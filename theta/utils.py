#!/usr/bin/env python3
"""
é€šç”¨å·¥å…·å‡½æ•° - ä¸ºæ‰€æœ‰SDVæ¨¡å‹æä¾›ç»Ÿä¸€çš„æ•°æ®å¤„ç†å’Œè¯„ä¼°åŠŸèƒ½
åˆå¹¶äº†enhanced_utils.pyçš„æ™ºèƒ½æ•°æ®å¤„ç†åŠŸèƒ½
"""

import pandas as pd
import numpy as np
import os
import time
from datetime import datetime, timedelta
from typing import Optional, Dict, Any, Tuple, List
from sdv.metadata import SingleTableMetadata
from sdv.evaluation.single_table import evaluate_quality, get_column_plot
import warnings
warnings.filterwarnings('ignore')

# è¿›åº¦æ¡æ”¯æŒ
try:
    from tqdm import tqdm
    TQDM_AVAILABLE = True
except ImportError:
    TQDM_AVAILABLE = False
    print("âš ï¸ tqdmæœªå®‰è£…ï¼Œå°†ä½¿ç”¨ç®€å•è¿›åº¦æ˜¾ç¤º")

def progress_bar(iterable=None, total=None, desc="Processing", **kwargs):
    """ç»Ÿä¸€çš„è¿›åº¦æ¡æ¥å£"""
    if TQDM_AVAILABLE:
        if iterable is not None:
            return tqdm(iterable, desc=desc, **kwargs)
        else:
            return tqdm(total=total, desc=desc, **kwargs)
    else:
        # ç®€å•çš„è¿›åº¦æ˜¾ç¤ºç±»
        class SimpleProgress:
            def __init__(self, total=None, desc="Processing"):
                self.total = total
                self.desc = desc
                self.n = 0
                self.start_time = time.time()
                
            def update(self, n=1):
                self.n += n
                if self.total:
                    percent = (self.n / self.total) * 100
                    elapsed = time.time() - self.start_time
                    print(f"\r{self.desc}: {percent:.1f}% ({self.n}/{self.total}) | è€—æ—¶: {elapsed:.1f}s", end="", flush=True)
                else:
                    elapsed = time.time() - self.start_time
                    print(f"\r{self.desc}: {self.n} é¡¹å®Œæˆ | è€—æ—¶: {elapsed:.1f}s", end="", flush=True)
                    
            def close(self):
                print()  # æ¢è¡Œ
                
            def __enter__(self):
                return self
                
            def __exit__(self, *args):
                self.close()
        
        if iterable is not None:
            # å¯¹äºå¯è¿­ä»£å¯¹è±¡ï¼Œåˆ›å»ºä¸€ä¸ªåŒ…è£…å™¨
            class IterableWrapper:
                def __init__(self, iterable, desc):
                    self.iterable = iterable
                    self.desc = desc
                    
                def __iter__(self):
                    with SimpleProgress(total=len(self.iterable) if hasattr(self.iterable, '__len__') else None, desc=self.desc) as pbar:
                        for item in self.iterable:
                            yield item
                            pbar.update(1)
            return IterableWrapper(iterable, desc)
        else:
            return SimpleProgress(total=total, desc=desc)

def load_data(file_path, **kwargs):
    """æ™ºèƒ½æ•°æ®åŠ è½½ï¼Œæ”¯æŒå¤šç§æ ¼å¼å’Œç¼–ç """
    print(f"ğŸ“Š åŠ è½½æ•°æ®: {file_path}")
    
    file_ext = os.path.splitext(file_path)[1].lower()
    
    try:
        if file_ext == '.csv':
            # å°è¯•å¤šç§ç¼–ç å’Œåˆ†éš”ç¬¦
            encodings = ['utf-8', 'gbk', 'gb2312', 'latin-1']
            separators = [',', ';', '\t']
            
            for encoding in progress_bar(encodings, desc="å°è¯•ç¼–ç "):
                for sep in separators:
                    try:
                        df = pd.read_csv(file_path, encoding=encoding, sep=sep, low_memory=False, **kwargs)
                        if df.shape[1] > 1:  # ç¡®ä¿æ­£ç¡®è§£æäº†åˆ—
                            print(f"ğŸ“Š âœ“ æˆåŠŸåŠ è½½ (ç¼–ç : {encoding}, åˆ†éš”ç¬¦: '{sep}')")
                            break
                    except:
                        continue
                else:
                    continue
                break
            else:
                raise ValueError("æ— æ³•ä½¿ç”¨ä»»ä½•ç¼–ç å’Œåˆ†éš”ç¬¦ç»„åˆåŠ è½½CSVæ–‡ä»¶")
                
        elif file_ext in ['.xlsx', '.xls']:
            df = pd.read_excel(file_path, **kwargs)
        elif file_ext == '.json':
            df = pd.read_json(file_path, **kwargs)
        elif file_ext == '.parquet':
            df = pd.read_parquet(file_path, **kwargs)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_ext}")
            
    except Exception as e:
        raise ValueError(f"æ•°æ®åŠ è½½å¤±è´¥: {e}")
    
    print(f"ğŸ“Š âœ“ æ•°æ®åŠ è½½å®Œæˆ: {df.shape}")
    print(f"ğŸ“Š âœ“ åˆ—å: {list(df.columns)[:10]}{'...' if len(df.columns) > 10 else ''}")
    
    # åŸºæœ¬æ•°æ®è´¨é‡æ£€æŸ¥
    _basic_data_quality_check(df)
    
    return df

def _basic_data_quality_check(df):
    """åŸºæœ¬æ•°æ®è´¨é‡æ£€æŸ¥"""
    total_cells = df.shape[0] * df.shape[1]
    missing_cells = df.isnull().sum().sum()
    missing_rate = missing_cells / total_cells * 100
    
    print(f"ğŸ“Š âœ“ æ•°æ®è´¨é‡æ£€æŸ¥: ç¼ºå¤±ç‡ {missing_rate:.2f}%")
    
    if missing_rate > 50:
        print("ğŸ“Š âš ï¸ è­¦å‘Š: æ•°æ®ç¼ºå¤±ç‡è¶…è¿‡50%ï¼Œå¯èƒ½å½±å“æ¨¡å‹æ€§èƒ½")
    
    # æ£€æŸ¥é‡å¤è¡Œ
    duplicates = df.duplicated().sum()
    if duplicates > 0:
        print(f"ğŸ“Š âš ï¸ å‘ç° {duplicates} è¡Œé‡å¤æ•°æ®")

def print_model_info(model_name, description):
    """æ‰“å°æ¨¡å‹ä¿¡æ¯"""
    print("=" * 60)
    print(f"æ¨¡å‹: {model_name}")
    print(f"æè¿°: {description}")
    print("=" * 60)

def select_single_user(df, user_id=169):
    """é€‰æ‹©å•ä¸ªç”¨æˆ·çš„æ•°æ®"""
    if 'user_id' not in df.columns:
        print("ğŸ“Š âš ï¸ æœªæ‰¾åˆ°user_idåˆ—ï¼Œä¿ç•™æ‰€æœ‰æ•°æ®")
        return df
    
    user_data = df[df['user_id'] == user_id].copy()
    if len(user_data) == 0:
        print(f"ğŸ“Š âš ï¸ æœªæ‰¾åˆ°ç”¨æˆ·{user_id}çš„æ•°æ®ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªç”¨æˆ·çš„æ•°æ®")
        first_user = df['user_id'].iloc[0]
        user_data = df[df['user_id'] == first_user].copy()
        user_id = first_user
    
    print(f"ğŸ“Š âœ“ é€‰æ‹©ç”¨æˆ·{user_id}çš„æ•°æ®: {len(user_data)}æ¡è®°å½•")
    return user_data

def preprocess_data(df, user_id=169, sample_size=None, reduce_cardinality=False, strategy='frequency_based'):
    """
    æ™ºèƒ½æ•°æ®é¢„å¤„ç† - é’ˆå¯¹å•ç”¨æˆ·æ•°æ®ä¼˜åŒ–
    
    Args:
        df: åŸå§‹DataFrame
        user_id: ç”¨æˆ·IDï¼Œç”¨äºå•ç”¨æˆ·æ•°æ®å¤„ç†
        sample_size: é‡‡æ ·å¤§å°ï¼Œå¦‚æœNoneåˆ™ä¸é‡‡æ ·
        reduce_cardinality: æ˜¯å¦å‡å°‘é«˜åŸºæ•°åˆ—ï¼ˆé€‚ç”¨äºGANæ¨¡å‹ï¼‰
        strategy: indicatorå¤„ç†ç­–ç•¥ ('frequency_based', 'adaptive', 'simple')
    
    Returns:
        é¢„å¤„ç†åçš„DataFrame
    """
    print(f"ğŸ“Š å¼€å§‹å•ç”¨æˆ·æ™ºèƒ½é¢„å¤„ç† (ç”¨æˆ·ID: {user_id}, ç­–ç•¥: {strategy})")
    original_shape = df.shape
    
    # 1. é€‰æ‹©å•ç”¨æˆ·æ•°æ®
    df_processed = select_single_user(df, user_id)
    
    # 2. æ•°æ®é‡‡æ ·ï¼ˆå¦‚æœéœ€è¦ï¼‰
    if sample_size and sample_size < len(df_processed):
        df_processed = df_processed.sample(n=sample_size, random_state=42)
        print(f"ğŸ“Š âœ“ æ•°æ®é‡‡æ ·: {len(df_processed)} è¡Œ")
    
    # 3. å¤„ç†ç¼ºå¤±å€¼
    df_processed = _handle_missing_values(df_processed)
    
    # 4. ä¿æŠ¤æ•°å€¼å­—æ®µï¼ˆç‰¹åˆ«æ˜¯valueå­—æ®µï¼‰
    df_processed = _protect_numeric_fields(df_processed)
    
    # 5. å¤„ç†æ•°æ®ç±»å‹
    df_processed = _optimize_data_types(df_processed)
    
    # 6. å¤„ç†é«˜åŸºæ•°åˆ—ï¼ˆå¦‚æœéœ€è¦ï¼Œä½†ä¿æŠ¤é‡è¦çš„æ•°å€¼å­—æ®µï¼‰
    if reduce_cardinality:
        df_processed = _reduce_cardinality_smart(df_processed, strategy)
    
    # 7. å¤„ç†æ—¶é—´åˆ—
    df_processed = _handle_time_columns(df_processed)
    
    # 8. å¤„ç†å¼‚å¸¸å€¼ï¼ˆåªå¯¹éä¿æŠ¤å­—æ®µï¼‰
    df_processed = _handle_outliers_smart(df_processed)
    
    # 9. æœ€ç»ˆæ¸…ç†
    df_processed = _final_cleanup(df_processed)
    
    print(f"ğŸ“Š âœ“ å•ç”¨æˆ·é¢„å¤„ç†å®Œæˆ: {original_shape} â†’ {df_processed.shape}")
    
    return df_processed

def _handle_missing_values(df):
    """å¤„ç†ç¼ºå¤±å€¼"""
    print("ğŸ“Š å¤„ç†ç¼ºå¤±å€¼...")
    
    # åˆ é™¤ç¼ºå¤±ç‡è¶…è¿‡80%çš„åˆ—
    missing_threshold = 0.8
    high_missing_cols = df.columns[df.isnull().mean() > missing_threshold].tolist()
    if high_missing_cols:
        print(f"ğŸ“Š âœ“ åˆ é™¤é«˜ç¼ºå¤±ç‡åˆ—: {len(high_missing_cols)}ä¸ª")
        df = df.drop(columns=high_missing_cols)
    
    # å¡«å……æ•°å€¼åˆ—çš„ç¼ºå¤±å€¼
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    for col in progress_bar(numeric_cols, desc="å¡«å……æ•°å€¼åˆ—"):
        if df[col].isnull().any():
            df[col] = df[col].fillna(df[col].median())
    
    # å¡«å……åˆ†ç±»åˆ—çš„ç¼ºå¤±å€¼
    categorical_cols = df.select_dtypes(include=['object']).columns
    for col in progress_bar(categorical_cols, desc="å¡«å……åˆ†ç±»åˆ—"):
        if df[col].isnull().any():
            mode_value = df[col].mode()
            if len(mode_value) > 0:
                df[col] = df[col].fillna(mode_value[0])
            else:
                df[col] = df[col].fillna('unknown')
    
    return df

def _protect_numeric_fields(df):
    """æ™ºèƒ½ä¿æŠ¤å’Œå¤„ç†æ•°å€¼å­—æ®µï¼Œæ ¹æ®indicatorç±»å‹ä¼˜åŒ–valueå­—æ®µ"""
    print("ğŸ“Š æ™ºèƒ½ä¿æŠ¤å’Œå¤„ç†æ•°å€¼å­—æ®µ...")
    
    # å®šä¹‰éœ€è¦ä¿æŠ¤çš„æ•°å€¼å­—æ®µ
    protected_fields = ['value', 'user_id', 'id']
    
    for field in protected_fields:
        if field in df.columns:
            # ç¡®ä¿è¿™äº›å­—æ®µä¿æŒä¸ºæ•°å€¼ç±»å‹
            if field == 'value':
                # valueå­—æ®µæ™ºèƒ½å¤„ç†ï¼šæ ¹æ®indicatorç±»å‹è¿›è¡Œä¸åŒçš„å¤„ç†
                df = _smart_process_value_field(df)
                print(f"ğŸ“Š âœ“ æ™ºèƒ½å¤„ç†æ•°å€¼å­—æ®µ {field}: {df[field].dtype}")
            elif field in ['user_id', 'id']:
                # IDå­—æ®µç¡®ä¿ä¸ºæ•´æ•°
                df[field] = pd.to_numeric(df[field], errors='coerce').fillna(0).astype('int64')
                print(f"ğŸ“Š âœ“ ä¿æŠ¤IDå­—æ®µ {field}: {df[field].dtype}")
    
    return df

def _smart_process_value_field(df):
    """æ ¹æ®indicatorç±»å‹æ™ºèƒ½å¤„ç†valueå­—æ®µ"""
    if 'indicator' not in df.columns or 'value' not in df.columns:
        return df
    
    print("ğŸ“Š æ ¹æ®indicatorç±»å‹æ™ºèƒ½å¤„ç†valueå­—æ®µ...")
    
    # å…ˆå°è¯•è½¬æ¢ä¸ºæ•°å€¼
    df['value'] = pd.to_numeric(df['value'], errors='coerce')
    
    # å¤„ç†JSONæ ¼å¼çš„value
    mask = df['value'].isna()
    if mask.any():
        for idx in df.index[mask]:
            original_value = str(df.at[idx, 'value'])
            if 'primary_value' in original_value:
                try:
                    import json
                    value_dict = json.loads(original_value.replace("'", '"'))
                    if 'primary_value' in value_dict:
                        df.at[idx, 'value'] = float(value_dict['primary_value'])
                except:
                    df.at[idx, 'value'] = 0.0
            else:
                df.at[idx, 'value'] = 0.0
    
    # æ ¹æ®indicatorç±»å‹è¿›è¡Œç‰¹æ®Šå¤„ç†
    for idx in df.index:
        indicator = str(df.at[idx, 'indicator']).lower()
        value = df.at[idx, 'value']
        
        if pd.isna(value):
            continue
            
        # 1. æ—¶é—´ç›¸å…³æŒ‡æ ‡ç»Ÿä¸€å¤„ç†
        if any(keyword in indicator for keyword in ['_start_time', '_end_time', 'sleep_start', 'sleep_end', '_time_', 'time_avg', 'avg_start', 'avg_end']):
            if value > 1000000000:  # Unixæ—¶é—´æˆ³
                try:
                    import datetime
                    dt = datetime.datetime.fromtimestamp(value)
                    df.at[idx, 'value'] = dt.hour  # è½¬æ¢ä¸ºå°æ—¶ (0-23)
                    print(f"ğŸ“Š   æ—¶é—´æˆ³è½¬æ¢: {indicator} {value} â†’ å°æ—¶{dt.hour}")
                except:
                    df.at[idx, 'value'] = 12  # é»˜è®¤ä¸­åˆ12ç‚¹
            elif value > 86400:  # è¶…è¿‡24å°æ—¶çš„ç§’æ•°ï¼Œè½¬æ¢ä¸ºå°æ—¶
                df.at[idx, 'value'] = (value % 86400) / 3600  # è½¬æ¢ä¸ºå°æ—¶
            elif value > 24:  # å¯èƒ½æ˜¯å°æ—¶ä½†è¶…å‡ºèŒƒå›´
                df.at[idx, 'value'] = value % 24
            elif value < 0:
                df.at[idx, 'value'] = 0
        
        # 2. è¡€æ°§ç›¸å…³ï¼Œé™åˆ¶åœ¨åˆç†èŒƒå›´ (90-100)
        elif 'blood_oxygen' in indicator or 'oxygen' in indicator:
            if value > 100:
                df.at[idx, 'value'] = min(100, max(90, value % 100 + 90))
            elif value < 50:
                df.at[idx, 'value'] = max(90, 95 + (value % 10))
        
        # 3. å¿ƒç‡ç›¸å…³ï¼Œé™åˆ¶åœ¨åˆç†èŒƒå›´ (40-200)
        elif 'heart_rate' in indicator or 'hr_' in indicator:
            if value > 200:
                df.at[idx, 'value'] = min(200, max(40, value % 160 + 40))
            elif value < 30:
                df.at[idx, 'value'] = max(40, 60 + (value % 20))
        
        # 4. ç™¾åˆ†æ¯”ç±»å‹ï¼Œé™åˆ¶åœ¨0-100
        elif any(keyword in indicator for keyword in ['percentage', 'percent', 'ratio']):
            if value > 100:
                df.at[idx, 'value'] = value % 100
            elif value < 0:
                df.at[idx, 'value'] = abs(value) % 100
        
        # 5. æ­¥æ•°ç›¸å…³ï¼Œé™åˆ¶åœ¨åˆç†èŒƒå›´ (0-50000)
        elif 'steps' in indicator:
            if value > 50000:
                df.at[idx, 'value'] = value % 50000
            elif value < 0:
                df.at[idx, 'value'] = abs(value) % 50000
        
        # 6. æ—¶é•¿ç±»å‹(ç§’)ï¼Œè½¬æ¢ä¸ºåˆ†é’Ÿå¹¶é™åˆ¶åˆç†èŒƒå›´
        elif 'duration' in indicator:
            if value > 86400:  # è¶…è¿‡24å°æ—¶çš„ç§’æ•°
                df.at[idx, 'value'] = min(1440, (value % 86400) / 60)  # è½¬æ¢ä¸ºåˆ†é’Ÿï¼Œæœ€å¤§24å°æ—¶
            elif value > 3600:  # è¶…è¿‡1å°æ—¶
                df.at[idx, 'value'] = min(1440, value / 60)  # è½¬æ¢ä¸ºåˆ†é’Ÿï¼Œæœ€å¤§24å°æ—¶
            elif value > 1440:  # å¦‚æœå·²ç»æ˜¯åˆ†é’Ÿä½†è¶…è¿‡24å°æ—¶
                df.at[idx, 'value'] = value % 1440  # é™åˆ¶åœ¨24å°æ—¶å†…
            elif value < 0:
                df.at[idx, 'value'] = 0
        
        # 7. è®¡æ•°ç±»å‹ï¼Œç¡®ä¿ä¸ºåˆç†æ•´æ•°
        elif any(keyword in indicator for keyword in ['count', '_days_', 'frequency']):
            if value > 365:  # è¶…è¿‡ä¸€å¹´
                df.at[idx, 'value'] = value % 365
            elif value < 0:
                df.at[idx, 'value'] = 0
            df.at[idx, 'value'] = int(df.at[idx, 'value'])
        
        # 8. è·ç¦»ç›¸å…³ï¼Œé™åˆ¶åœ¨åˆç†èŒƒå›´(ç±³)
        elif 'distance' in indicator:
            if value > 100000:  # è¶…è¿‡100å…¬é‡Œ
                df.at[idx, 'value'] = value % 100000
            elif value < 0:
                df.at[idx, 'value'] = 0
        
        # 9. VO2 maxç›¸å…³ï¼Œé™åˆ¶åœ¨åˆç†èŒƒå›´
        elif 'vo2' in indicator:
            if value > 100:
                df.at[idx, 'value'] = min(80, max(10, value % 80 + 10))
            elif value < 0:
                df.at[idx, 'value'] = max(10, 30 + (abs(value) % 20))
    
    # ç¡®ä¿æ‰€æœ‰valueéƒ½æ˜¯æœ‰é™æ•°å€¼
    df['value'] = df['value'].fillna(0)
    df['value'] = df['value'].replace([np.inf, -np.inf], 0)
    
    # ç»Ÿè®¡å¤„ç†ç»“æœ
    value_stats = df['value'].describe()
    print(f"ğŸ“Š Valueå­—æ®µå¤„ç†å®Œæˆ:")
    print(f"ğŸ“Š   èŒƒå›´: {value_stats['min']:.2f} - {value_stats['max']:.2f}")
    print(f"ğŸ“Š   å¹³å‡: {value_stats['mean']:.2f}")
    print(f"ğŸ“Š   ä¸­ä½æ•°: {value_stats['50%']:.2f}")
    
    return df

def _optimize_data_types(df):
    """ä¼˜åŒ–æ•°æ®ç±»å‹"""
    print("ğŸ“Š ä¼˜åŒ–æ•°æ®ç±»å‹...")
    
    for col in progress_bar(df.columns, desc="ä¼˜åŒ–æ•°æ®ç±»å‹"):
        if df[col].dtype == 'object':
            # å°è¯•è½¬æ¢ä¸ºæ•°å€¼ç±»å‹
            numeric_data = pd.to_numeric(df[col], errors='coerce')
            if not numeric_data.isna().all():
                df[col] = numeric_data
            else:
                # å°è¯•è½¬æ¢ä¸ºdatetime
                try:
                    datetime_data = pd.to_datetime(df[col], errors='coerce')
                    if not datetime_data.isna().all():
                        df[col] = datetime_data
                    else:
                        # è½¬æ¢ä¸ºcategoryç±»å‹ï¼ˆå†…å­˜ä¼˜åŒ–ï¼‰
                        if df[col].nunique() < len(df) * 0.5:
                            df[col] = df[col].astype('category')
                except:
                    pass
        
        # ä¼˜åŒ–æ•´æ•°ç±»å‹
        elif df[col].dtype in ['int64', 'int32']:
            min_val, max_val = df[col].min(), df[col].max()
            if min_val >= 0 and max_val < 256:
                df[col] = df[col].astype('uint8')
            elif min_val >= -128 and max_val < 128:
                df[col] = df[col].astype('int8')
            elif min_val >= -32768 and max_val < 32768:
                df[col] = df[col].astype('int16')
        
        # ä¼˜åŒ–æµ®ç‚¹ç±»å‹
        elif df[col].dtype == 'float64':
            df[col] = pd.to_numeric(df[col], downcast='float')
    
    return df

def _reduce_cardinality_smart(df, strategy):
    """æ™ºèƒ½å‡å°‘é«˜åŸºæ•°åˆ—ï¼Œä¿æŠ¤é‡è¦çš„æ•°å€¼å­—æ®µ"""
    print(f"ğŸ“Š æ™ºèƒ½å‡å°‘é«˜åŸºæ•°åˆ— (ç­–ç•¥: {strategy})...")
    
    # å®šä¹‰ä¸åº”è¯¥è¿›è¡ŒåŸºæ•°å‡å°‘çš„å­—æ®µ
    protected_fields = ['value', 'user_id', 'id']
    
    for col in progress_bar(df.columns, desc="å¤„ç†é«˜åŸºæ•°åˆ—"):
        # è·³è¿‡ä¿æŠ¤çš„æ•°å€¼å­—æ®µ
        if col in protected_fields:
            continue
            
        if df[col].dtype == 'object' or df[col].dtype.name == 'category':
            unique_count = df[col].nunique()
            
            # å¦‚æœå”¯ä¸€å€¼è¿‡å¤šï¼Œè¿›è¡Œå¤„ç†
            if unique_count > 50:  # é™ä½é˜ˆå€¼ï¼Œå› ä¸ºæ˜¯å•ç”¨æˆ·æ•°æ®
                # å…ˆè½¬æ¢ä¸ºobjectç±»å‹ä»¥é¿å…categoryç±»å‹çš„é™åˆ¶
                if df[col].dtype.name == 'category':
                    df[col] = df[col].astype('object')
                
                if strategy == 'frequency_based':
                    # åŸºäºé¢‘ç‡ä¿ç•™å‰Nä¸ªå€¼
                    top_values = df[col].value_counts().head(30).index  # å‡å°‘ä¿ç•™æ•°é‡
                    df.loc[~df[col].isin(top_values), col] = 'other'
                    
                elif strategy == 'adaptive':
                    # è‡ªé€‚åº”ç­–ç•¥ï¼šä¿ç•™è¦†ç›–90%æ•°æ®çš„å€¼
                    value_counts = df[col].value_counts()
                    cumsum = value_counts.cumsum()
                    threshold = len(df) * 0.9
                    keep_values = value_counts[cumsum <= threshold].index
                    df.loc[~df[col].isin(keep_values), col] = 'other'
                    
                elif strategy == 'simple':
                    # ç®€å•ç­–ç•¥ï¼šä¿ç•™å‰15ä¸ªæœ€å¸¸è§çš„å€¼
                    top_values = df[col].value_counts().head(15).index
                    df.loc[~df[col].isin(top_values), col] = 'other'
                
                new_unique_count = df[col].nunique()
                print(f"ğŸ“Š   {col}: {unique_count} â†’ {new_unique_count} ä¸ªå”¯ä¸€å€¼")
    
    return df

def _handle_time_columns(df):
    """æ™ºèƒ½å¤„ç†æ—¶é—´åˆ—"""
    print("ğŸ“Š å¤„ç†æ—¶é—´åˆ—...")
    
    time_columns = ['start_time', 'end_time', 'create_time', 'update_time']
    for col in progress_bar(time_columns, desc="å¤„ç†æ—¶é—´åˆ—"):
        if col in df.columns:
            try:
                # è½¬æ¢ä¸ºdatetime
                df[col] = pd.to_datetime(df[col], errors='coerce')
                
                # æ£€æŸ¥æœ‰æ•ˆæ•°æ®çš„æ¯”ä¾‹
                valid_rate = df[col].notna().sum() / len(df)
                if valid_rate < 0.5:
                    # å¦‚æœå¤§éƒ¨åˆ†æ•°æ®æ— æ•ˆï¼Œåˆ é™¤è¯¥åˆ—
                    df = df.drop(columns=[col])
                    continue
                
                # æå–æ—¶é—´ç‰¹å¾
                if col in ['start_time', 'end_time']:
                    df[f'{col}_year'] = df[col].dt.year.fillna(2023)
                    df[f'{col}_month'] = df[col].dt.month.fillna(1)
                    df[f'{col}_day'] = df[col].dt.day.fillna(1)
                    df[f'{col}_hour'] = df[col].dt.hour.fillna(0)
                    df[f'{col}_weekday'] = df[col].dt.dayofweek.fillna(0)
                
                # åˆ é™¤åŸå§‹æ—¶é—´åˆ—
                df = df.drop(columns=[col])
                
            except Exception as e:
                print(f"ğŸ“Š   {col} å¤„ç†å¤±è´¥: {e}")
                df = df.drop(columns=[col], errors='ignore')
    
    return df

def _handle_outliers_smart(df):
    """æ™ºèƒ½å¤„ç†å¼‚å¸¸å€¼ï¼Œä¿æŠ¤é‡è¦å­—æ®µ"""
    print("ğŸ“Š æ™ºèƒ½å¤„ç†å¼‚å¸¸å€¼...")
    
    # å®šä¹‰ä¸å¤„ç†å¼‚å¸¸å€¼çš„å­—æ®µ
    protected_fields = ['value', 'user_id', 'id']
    
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    
    for col in progress_bar(numeric_cols, desc="å¤„ç†å¼‚å¸¸å€¼"):
        # è·³è¿‡ä¿æŠ¤çš„å­—æ®µ
        if col in protected_fields:
            continue
            
        if df[col].std() == 0:  # è·³è¿‡å¸¸æ•°åˆ—
            continue
            
        # ä½¿ç”¨IQRæ–¹æ³•è¯†åˆ«å¼‚å¸¸å€¼
        Q1 = df[col].quantile(0.25)
        Q3 = df[col].quantile(0.75)
        IQR = Q3 - Q1
        
        if IQR > 0:  # é¿å…é™¤é›¶é”™è¯¯
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR
            
            # è®¡ç®—å¼‚å¸¸å€¼æ¯”ä¾‹
            outliers = (df[col] < lower_bound) | (df[col] > upper_bound)
            outlier_ratio = outliers.sum() / len(df)
            
            # å¦‚æœå¼‚å¸¸å€¼æ¯”ä¾‹ä¸å¤ªé«˜ï¼ˆ<10%ï¼‰ï¼Œè¿›è¡Œå¤„ç†
            if 0 < outlier_ratio < 0.1:
                # ä½¿ç”¨è¾¹ç•Œå€¼æ›¿æ¢å¼‚å¸¸å€¼
                df.loc[df[col] < lower_bound, col] = lower_bound
                df.loc[df[col] > upper_bound, col] = upper_bound
                print(f"ğŸ“Š   {col}: å¤„ç†äº† {outliers.sum()} ä¸ªå¼‚å¸¸å€¼ ({outlier_ratio*100:.1f}%)")
    
    return df

def _final_cleanup(df):
    """æœ€ç»ˆæ¸…ç†"""
    print("ğŸ“Š æœ€ç»ˆæ•°æ®æ¸…ç†...")
    
    # åˆ é™¤å®Œå…¨ç©ºçš„è¡Œ
    df = df.dropna(how='all')
    
    # ç¡®ä¿æ‰€æœ‰categoryç±»å‹è½¬æ¢ä¸ºobjectç±»å‹ï¼ˆå…¼å®¹æ‰€æœ‰SDVæ¨¡å‹ï¼‰
    for col in df.columns:
        if df[col].dtype.name == 'category':
            df[col] = df[col].astype('object')
    
    # é‡ç½®ç´¢å¼•
    df = df.reset_index(drop=True)
    
    return df

def create_metadata(df):
    """åˆ›å»ºå…ƒæ•°æ®"""
    print("ğŸ“Š æ­£åœ¨åˆ›å»ºå…ƒæ•°æ®...")
    metadata = SingleTableMetadata()
    metadata.detect_from_dataframe(df)
    
    print("ğŸ“Š å…ƒæ•°æ®åˆ›å»ºå®Œæˆ")
    return metadata

def preprocess_sequential_data(df, user_col='user_id', target_user_id=169, sample_size=None):
    """ä¸“é—¨ä¸ºåºåˆ—æ¨¡å‹é¢„å¤„ç†æ•°æ®"""
    print("ğŸ“Š å¼€å§‹åºåˆ—æ•°æ®é¢„å¤„ç†...")
    original_shape = df.shape
    
    # 1. æ•°æ®æ¸…ç†
    df_processed = df.copy()
    
    # åˆ é™¤æ— ç”¨åˆ—
    columns_to_drop = ['id', 'source_table_id', 'comment', 'indicator_id']
    for col in columns_to_drop:
        if col in df_processed.columns:
            df_processed = df_processed.drop(columns=[col])
    
    # 2. é€‰æ‹©å•ä¸ªç”¨æˆ·çš„æ•°æ®
    if user_col in df_processed.columns and target_user_id:
        user_data = df_processed[df_processed[user_col] == target_user_id].copy()
        if len(user_data) > 0:
            df_processed = user_data
            print(f"ğŸ“Š âœ“ é€‰æ‹©ç”¨æˆ·{target_user_id}çš„æ•°æ®: {len(df_processed)}æ¡è®°å½•")
        else:
            print(f"ğŸ“Š âš ï¸ æœªæ‰¾åˆ°ç”¨æˆ·{target_user_id}çš„æ•°æ®ï¼Œä½¿ç”¨æ‰€æœ‰ç”¨æˆ·æ•°æ®")
    
    # 3. å¤„ç†æ—¶é—´åˆ—
    df_processed = _handle_sequential_time_columns(df_processed)
    
    # 4. æ•°æ®é‡‡æ ·
    if sample_size and sample_size < len(df_processed):
        df_processed = df_processed.sample(n=sample_size, random_state=42)
        print(f"ğŸ“Š âœ“ æ•°æ®é‡‡æ ·: {len(df_processed)} è¡Œ")
    
    # 5. ä¿æŠ¤æ•°å€¼å­—æ®µï¼ˆç‰¹åˆ«æ˜¯valueå­—æ®µï¼‰
    df_processed = _protect_numeric_fields(df_processed)
    
    # 6. å¤„ç†é«˜åŸºæ•°åˆ—ï¼ˆæ™ºèƒ½ä¿æŠ¤ï¼‰
    df_processed = _reduce_cardinality_smart(df_processed, 'frequency_based')
    
    # 7. å¤„ç†ç¼ºå¤±å€¼
    df_processed = _handle_missing_values(df_processed)
    
    # 8. åˆ›å»ºåºåˆ—ç´¢å¼•
    df_processed = _create_sequence_index(df_processed, user_col)
    
    # 9. æœ€ç»ˆæ¸…ç†
    df_processed = _final_cleanup(df_processed)
    
    print(f"ğŸ“Š âœ“ åºåˆ—é¢„å¤„ç†å®Œæˆ: {original_shape} â†’ {df_processed.shape}")
    
    return df_processed

def _handle_sequential_time_columns(df):
    """å¤„ç†åºåˆ—æ•°æ®çš„æ—¶é—´åˆ—"""
    print("ğŸ“Š å¤„ç†åºåˆ—æ—¶é—´åˆ—...")
    
    # ä¼˜å…ˆä½¿ç”¨create_timeï¼Œå› ä¸ºå®ƒçš„æ•°æ®å®Œæ•´æ€§æ›´å¥½
    time_columns = ['create_time', 'start_time', 'update_time', 'end_time']
    
    for time_col in time_columns:
        if time_col in df.columns:
            try:
                df[time_col] = pd.to_datetime(df[time_col], errors='coerce')
                
                # æ£€æŸ¥æœ‰æ•ˆæ€§
                valid_count = df[time_col].notna().sum()
                if valid_count > len(df) * 0.8:  # å¦‚æœ80%ä»¥ä¸Šçš„æ•°æ®æœ‰æ•ˆ
                    # åˆ›å»ºè¿ç»­çš„åºåˆ—æ—¶é—´ç´¢å¼•
                    df = df.sort_values([time_col]).reset_index(drop=True)
                    df['sequence_datetime'] = df[time_col]
                    
                    # æå–æ—¶é—´ç‰¹å¾
                    df['hour'] = df[time_col].dt.hour
                    df['day_of_week'] = df[time_col].dt.dayofweek
                    df['month'] = df[time_col].dt.month
                    
                    print(f"ğŸ“Š âœ“ ä½¿ç”¨ {time_col} åˆ›å»ºåºåˆ—æ—¶é—´ç´¢å¼•")
                    break
                    
            except Exception as e:
                print(f"ğŸ“Š âš ï¸ {time_col} å¤„ç†å¤±è´¥: {e}")
                continue
    else:
        # å¦‚æœæ²¡æœ‰å¯ç”¨çš„æ—¶é—´åˆ—ï¼Œåˆ›å»ºäººå·¥åºåˆ—ç´¢å¼•
        df['sequence_datetime'] = pd.date_range('2023-01-01', periods=len(df), freq='H')
        print("ğŸ“Š âš ï¸ åˆ›å»ºäººå·¥åºåˆ—æ—¶é—´ç´¢å¼•")
    
    return df

def _create_sequence_index(df, user_col):
    """åˆ›å»ºåºåˆ—ç´¢å¼•"""
    print("ğŸ“Š åˆ›å»ºåºåˆ—ç´¢å¼•...")
    
    if user_col in df.columns:
        # ä¸ºæ¯ä¸ªç”¨æˆ·åˆ›å»ºç‹¬ç«‹çš„åºåˆ—ç´¢å¼•
        def create_user_sequence(group):
            group = group.copy()
            group['sequence_index'] = range(len(group))
            return group
        
        df = df.groupby(user_col).apply(create_user_sequence).reset_index(drop=True)
    else:
        # åˆ›å»ºå…¨å±€åºåˆ—ç´¢å¼•
        df['sequence_index'] = range(len(df))
    
    return df

def create_sequential_metadata(df):
    """åˆ›å»ºåºåˆ—æ•°æ®å…ƒæ•°æ®"""
    print("ğŸ“Š æ­£åœ¨åˆ›å»ºåºåˆ—å…ƒæ•°æ®...")
    
    metadata = SingleTableMetadata()
    metadata.detect_from_dataframe(df)
    
    # è®¾ç½®åºåˆ—é”®
    if 'user_id' in df.columns:
        metadata.update_column('user_id', sdtype='id')
        metadata.set_sequence_key('user_id')
        print("ğŸ“Š âœ“ è®¾ç½®åºåˆ—é”®ä¸º: user_id")
    
    # è®¾ç½®åºåˆ—ç´¢å¼•
    sequence_index = None
    for col in ['sequence_datetime', 'sequence_index', 'day_of_year']:
        if col in df.columns:
            if col == 'sequence_datetime':
                metadata.update_column(col, sdtype='datetime')
            else:
                metadata.update_column(col, sdtype='numerical')
            sequence_index = col
            break
    
    if sequence_index:
        metadata.set_sequence_index(sequence_index)
        print(f"ğŸ“Š âœ“ è®¾ç½®åºåˆ—ç´¢å¼•ä¸º: {sequence_index}")
    
    print(f"ğŸ“Š âœ“ åºåˆ—å…ƒæ•°æ®åˆ›å»ºå®Œæˆï¼Œsequence_key: user_id")
    return metadata

def train_and_sample(synthesizer, real_data, num_rows=1000):
    """è®­ç»ƒåˆæˆå™¨å¹¶ç”Ÿæˆæ•°æ®"""
    print("ğŸ“Š æ­£åœ¨è®­ç»ƒåˆæˆå™¨...")
    start_time = datetime.now()
    
    synthesizer.fit(real_data)
    
    train_time = datetime.now() - start_time
    print(f"ğŸ“Š è®­ç»ƒå®Œæˆï¼Œè€—æ—¶: {train_time}")
    
    print(f"ğŸ“Š æ­£åœ¨ç”Ÿæˆ {num_rows} è¡Œåˆæˆæ•°æ®...")
    synthetic_data = synthesizer.sample(num_rows=num_rows)
    
    return synthetic_data, train_time

def evaluate_model(real_data, synthetic_data, metadata, model_name):
    """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
    print(f"ğŸ“Š æ­£åœ¨è¯„ä¼° {model_name} æ¨¡å‹...")
    
    try:
        quality_report = evaluate_quality(
            real_data=real_data,
            synthetic_data=synthetic_data,
            metadata=metadata
        )
        
        overall_score = quality_report.get_score()
        print(f"ğŸ“Š {model_name} æ€»ä½“è´¨é‡åˆ†æ•°: {overall_score*100:.2f}%")
        
        return quality_report
    except Exception as e:
        print(f"ğŸ“Š è¯„ä¼°è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        return None

def save_results(synthetic_data, model_name, output_dir, metadata=None, quality_report=None, train_time=None):
    """ä¿å­˜ç»“æœ"""
    os.makedirs(output_dir, exist_ok=True)
    
    # ä¿å­˜åˆæˆæ•°æ®
    output_file = os.path.join(output_dir, f'{model_name}_synthetic_data.csv')
    synthetic_data.to_csv(output_file, index=False)
    print(f"ğŸ“Š âœ“ åˆæˆæ•°æ®å·²ä¿å­˜è‡³: {output_file}")
    
    # ç”Ÿæˆå¹¶ä¿å­˜æ±‡æ€»æŠ¥å‘Š
    summary_file = os.path.join(output_dir, f'{model_name}_summary.txt')
    with open(summary_file, 'w', encoding='utf-8') as f:
        f.write(f"=== {model_name} æ¨¡å‹è®­ç»ƒç»“æœæ±‡æ€» ===\n\n")
        f.write(f"è®­ç»ƒæ—¶é—´: {train_time}\n")
        f.write(f"ç”Ÿæˆæ•°æ®è¡Œæ•°: {len(synthetic_data)}\n")
        f.write(f"ç”Ÿæˆæ•°æ®åˆ—æ•°: {len(synthetic_data.columns)}\n")
        
        if quality_report:
            overall_score = quality_report.get_score()
            f.write(f"æ€»ä½“è´¨é‡åˆ†æ•°: {overall_score*100:.2f}%\n")
            
            # è´¨é‡ç­‰çº§è¯„å®š
            if overall_score >= 0.9:
                quality_level = "å“è¶Š"
            elif overall_score >= 0.8:
                quality_level = "ä¼˜ç§€"
            elif overall_score >= 0.7:
                quality_level = "è‰¯å¥½"
            elif overall_score >= 0.6:
                quality_level = "ä¸€èˆ¬"
            else:
                quality_level = "éœ€è¦æ”¹è¿›"
            
            f.write(f"è´¨é‡ç­‰çº§: {quality_level}\n")
        
        f.write(f"\næ•°æ®æ ·ä¾‹:\n")
        f.write(synthetic_data.head().to_string())
        
        f.write(f"\n\næ•°æ®ç»Ÿè®¡:\n")
        f.write(synthetic_data.describe().to_string())
    
    print(f"ğŸ“Š âœ“ æ±‡æ€»æŠ¥å‘Šå·²ä¿å­˜è‡³: {summary_file}")
    return summary_file 