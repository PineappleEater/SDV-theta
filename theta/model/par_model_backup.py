#!/usr/bin/env python3
"""
PAR æ¨¡å‹ - åŸºäºæ¦‚ç‡è‡ªå›å½’çš„åºåˆ—æ•°æ®åˆæˆ
é€‚ç”¨åœºæ™¯: æ—¶é—´åºåˆ—æ•°æ®ï¼Œå…·æœ‰åºåˆ—ä¾èµ–å…³ç³»çš„æ•°æ®
å¢å¼ºæ¨¡å¼: ä¿æŒJSON/æ–‡æœ¬ç»“æ„çš„å¤æ‚åŒ»ç–—æ•°æ®å¤„ç†
"""

import sys
import os
import re
import pandas as pd
import numpy as np
from datetime import datetime
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from sdv.sequential import PARSynthesizer
from sdv.metadata import SingleTableMetadata
from utils import *

class SequentialTextProcessor:
    """åºåˆ—æ–‡æœ¬å¤„ç†å™¨ï¼Œä¿æŒæ—¶é—´åºåˆ—ä¸­çš„æ–‡æœ¬ç»“æ„"""
    
    def __init__(self):
        self.medical_terms = [
            'å¿ƒå¾‹', 'å¿ƒç”µå›¾', 'è¶…å£°', 'è¡€å‹', 'è¡€ç³–', 'ç”²çŠ¶è…º', 'ç»“èŠ‚', 
            'çª¦æ€§', 'å¿ƒåŠ¨', 'æ­£å¸¸', 'å¼‚å¸¸', 'æ£€æŸ¥', 'æŠ¥å‘Š', 'åˆ†æ'
        ]
    
    def create_sequence_features(self, df, value_col='value'):
        """ä¸ºåºåˆ—æ•°æ®åˆ›å»ºç»“æ„åŒ–ç‰¹å¾"""
        print("ğŸ—ï¸ åˆ›å»ºåºåˆ—ç»“æ„åŒ–ç‰¹å¾...")
        
        df_enhanced = df.copy()
        
        # æ–‡æœ¬åˆ†ç±»ç‰¹å¾
        df_enhanced['text_category'] = df[value_col].apply(self._classify_medical_text)
        df_enhanced['medical_term_count'] = df[value_col].apply(self._count_medical_terms)
        df_enhanced['text_complexity'] = df[value_col].apply(self._assess_complexity)
        df_enhanced['report_type'] = df[value_col].apply(self._classify_report_type)
        df_enhanced['has_measurements'] = df[value_col].apply(self._detect_measurements)
        
        # åºåˆ—ä½ç½®ç‰¹å¾
        df_enhanced['sequence_position'] = df_enhanced.groupby('user_id').cumcount()
        
        print(f"âœ“ åºåˆ—ç‰¹å¾åˆ›å»ºå®Œæˆ")
        return df_enhanced
    
    def _classify_medical_text(self, text):
        if pd.isna(text):
            return 'empty'
        text_str = str(text)
        
        if re.search(r'\d+\.\s*[^\n]+.*\n.*\d+\.', text_str):
            return 'detailed_report'
        elif any(term in text_str for term in ['å¿ƒç”µå›¾', 'è¶…å£°', 'è¡€å‹']):
            return 'diagnostic_test'
        elif 'æ­£å¸¸' in text_str:
            return 'normal_result'
        elif 'å¼‚å¸¸' in text_str:
            return 'abnormal_result'
        elif re.search(r'\d+(\.\d+)?\s*(mmol|mg|%)', text_str):
            return 'measurement'
        else:
            return 'general_text'
    
    def _count_medical_terms(self, text):
        if pd.isna(text):
            return 0
        text_str = str(text)
        return sum(1 for term in self.medical_terms if term in text_str)
    
    def _assess_complexity(self, text):
        if pd.isna(text):
            return 'simple'
        text_str = str(text)
        
        if len(text_str) > 50 and '\n' in text_str:
            return 'complex'
        elif len(text_str) > 20:
            return 'medium'
        else:
            return 'simple'
    
    def _classify_report_type(self, text):
        if pd.isna(text):
            return 'none'
        text_str = str(text)
        
        if 'å¿ƒç”µå›¾' in text_str:
            return 'ecg_report'
        elif 'è¶…å£°' in text_str:
            return 'ultrasound_report'
        elif 'è¡€' in text_str:
            return 'blood_test'
        elif re.search(r'\d+\.\s*', text_str):
            return 'structured_report'
        else:
            return 'general_report'
    
    def _detect_measurements(self, text):
        if pd.isna(text):
            return 0
        text_str = str(text)
        
        if re.search(r'\d+(\.\d+)?\s*(mmol|mg|%|æ¬¡|åˆ†é’Ÿ)', text_str):
            return 1
        else:
            return 0

def preprocess_enhanced(df, sample_size=3000, enhanced_mode=False):
    """å¢å¼ºé¢„å¤„ç†ï¼Œå¯é€‰æ‹©æ˜¯å¦ä¿æŒæ–‡æœ¬ç»“æ„"""
    if enhanced_mode:
        print("ğŸ”§ ä½¿ç”¨å¢å¼ºæ¨¡å¼é¢„å¤„ç†...")
        
        # æ™ºèƒ½é‡‡æ ·ä¿æŒåºåˆ—å®Œæ•´æ€§
        if sample_size and len(df) > sample_size:
            users = df['user_id'].unique()
            sample_users = np.random.choice(users, size=min(sample_size//20, len(users)), replace=False)
            df = df[df['user_id'].isin(sample_users)]
            print(f"âœ“ æŒ‰ç”¨æˆ·é‡‡æ ·ï¼Œä¿ç•™ {len(sample_users)} ä¸ªç”¨æˆ·çš„å®Œæ•´åºåˆ—")
        
        # åˆ é™¤ä¸å¿…è¦çš„åˆ—
        columns_to_drop = ['id', 'source_table_id', 'comment', 'indicator_id', 'deleted']
        for col in columns_to_drop:
            if col in df.columns:
                df = df.drop(columns=[col])
        
        # ä½¿ç”¨åºåˆ—æ–‡æœ¬å¤„ç†å™¨
        text_processor = SequentialTextProcessor()
        df_enhanced = text_processor.create_sequence_features(df)
        
        # ä¿ç•™æ›´å¤šåŒ»ç–—æŒ‡æ ‡
        if 'indicator' in df_enhanced.columns:
            top_indicators = df_enhanced['indicator'].value_counts().head(40).index
            df_enhanced.loc[~df_enhanced['indicator'].isin(top_indicators), 'indicator'] = 'other_medical_indicator'
            print(f"âœ“ ä¿ç•™å‰40ä¸ªé‡è¦åŒ»ç–—æŒ‡æ ‡")
        
        # ç®€åŒ–æ—¶é—´å¤„ç†
        if 'create_time' in df_enhanced.columns:
            try:
                time_data = pd.to_datetime(df_enhanced['create_time'])
                df_enhanced['create_time_hour'] = time_data.dt.hour
                df_enhanced = df_enhanced.drop(columns=['create_time'])
                print(f"âœ“ create_time è½¬æ¢ä¸ºå°æ—¶ç‰¹å¾")
            except:
                df_enhanced = df_enhanced.drop(columns=['create_time'])
        
        # åˆ é™¤å…¶ä»–æ—¶é—´åˆ—
        time_columns = ['start_time', 'end_time', 'update_time']
        for col in time_columns:
            if col in df_enhanced.columns:
                df_enhanced = df_enhanced.drop(columns=[col])
        
        # ä¿å­˜valueåˆ—æ ·æœ¬ç”¨äºé‡æ„
        if 'value' in df_enhanced.columns:
            df_enhanced['value_simplified'] = df_enhanced['value'].apply(
                lambda x: str(x)[:50] if pd.notna(x) else 'empty'
            )
            print("âœ“ ä¿å­˜valueåˆ—æ ·æœ¬ç”¨äºé‡æ„")
        
        return df_enhanced, text_processor
    else:
        # ä½¿ç”¨åŸå§‹é¢„å¤„ç†
        return preprocess_data(df, sample_size=sample_size, reduce_cardinality=True), None

def prepare_sequential_data(df, enhanced_mode=False):
    """ä¸ºPARæ¨¡å‹å‡†å¤‡åºåˆ—æ•°æ®"""
    print("æ­£åœ¨ä¸ºPARæ¨¡å‹å‡†å¤‡åºåˆ—æ•°æ®...")
    
    # ç¡®ä¿æœ‰ç”¨æˆ·IDåˆ—ç”¨äºåˆ†ç»„åºåˆ—
    if 'user_id' not in df.columns:
        print("âŒ æ•°æ®ä¸­æ²¡æœ‰user_idåˆ—ï¼ŒPARæ¨¡å‹éœ€è¦åºåˆ—æ ‡è¯†ç¬¦")
        return None
    
    # æ’åºæ•°æ®ä»¥ç¡®ä¿æ—¶é—´é¡ºåº
    if enhanced_mode:
        df = df.sort_values(['user_id', 'sequence_position'])
    else:
        time_cols = ['start_time', 'end_time', 'create_time', 'update_time']
        sort_col = None
        
        for col in time_cols:
            if col in df.columns:
                sort_col = col
                break
        
        if sort_col:
            # å…ˆè½¬æ¢ä¸ºæ•°å€¼æ—¶é—´æˆ³
            df[sort_col] = pd.to_datetime(df[sort_col], errors='coerce')
            df = df.sort_values(['user_id', sort_col])
            # è½¬æ¢ä¸ºUnixæ—¶é—´æˆ³
            df[sort_col] = df[sort_col].astype(int) // 10**9
            print(f"âœ“ å·²æŒ‰ user_id å’Œ {sort_col} æ’åº")
        else:
            df = df.sort_values('user_id')
            print("âœ“ å·²æŒ‰ user_id æ’åº")
    
    # æ£€æŸ¥æ¯ä¸ªç”¨æˆ·çš„åºåˆ—é•¿åº¦
    sequence_lengths = df.groupby('user_id').size()
    print(f"âœ“ ç”¨æˆ·æ•°é‡: {len(sequence_lengths)}")
    print(f"âœ“ å¹³å‡åºåˆ—é•¿åº¦: {sequence_lengths.mean():.1f}")
    print(f"âœ“ åºåˆ—é•¿åº¦èŒƒå›´: {sequence_lengths.min()} - {sequence_lengths.max()}")
    
    # è¿‡æ»¤æ‰åºåˆ—å¤ªçŸ­çš„ç”¨æˆ·
    min_length = 3 if enhanced_mode else 2
    valid_users = sequence_lengths[sequence_lengths >= min_length].index
    df_filtered = df[df['user_id'].isin(valid_users)]
    
    if len(df_filtered) < len(df):
        removed_count = len(df) - len(df_filtered)
        print(f"âœ“ å·²ç§»é™¤ {removed_count} æ¡è®°å½•ï¼ˆæ¥è‡ªåºåˆ—é•¿åº¦<{min_length}çš„ç”¨æˆ·ï¼‰")
    
    return df_filtered

def reconstruct_text_structure(synthetic_df, text_processor=None, enhanced_mode=False):
    """é‡æ„æ–‡æœ¬ç»“æ„"""
    if not enhanced_mode or text_processor is None:
        return synthetic_df
    
    print("ğŸ”„ é‡æ„åŒ»ç–—æ–‡æœ¬ç»“æ„...")
    synthetic_df = synthetic_df.copy()
    
    def reconstruct_medical_value(row):
        text_category = row.get('text_category', 'general_text')
        report_type = row.get('report_type', 'general_report')
        complexity = row.get('text_complexity', 'simple')
        medical_count = row.get('medical_term_count', 0)
        has_measurements = row.get('has_measurements', 0)
        
        if report_type == 'ecg_report':
            if complexity == 'complex':
                return "1.çª¦æ€§å¿ƒå¾‹\n2.å¿ƒç‡æ­£å¸¸\n3.æœªè§æ˜æ˜¾å¼‚å¸¸"
            else:
                return "çª¦æ€§å¿ƒå¾‹ï¼Œå¿ƒç‡æ­£å¸¸"
        elif report_type == 'ultrasound_report':
            if text_category == 'abnormal_result':
                return "è¶…å£°æ£€æŸ¥ï¼šå‘ç°ç»“èŠ‚ï¼Œå»ºè®®è¿›ä¸€æ­¥æ£€æŸ¥"
            else:
                return "è¶…å£°æ£€æŸ¥ï¼šå„è„å™¨å¤§å°å½¢æ€æ­£å¸¸"
        elif report_type == 'blood_test':
            if has_measurements:
                return "è¡€ç³–:5.2mmol/Lï¼Œæ€»èƒ†å›ºé†‡:4.1mmol/L"
            else:
                return "è¡€æ¶²æ£€æŸ¥å„é¡¹æŒ‡æ ‡æ­£å¸¸"
        elif report_type == 'structured_report':
            if medical_count > 2:
                return "1.æ£€æŸ¥å®Œæˆ\n2.å„é¡¹æŒ‡æ ‡åœ¨æ­£å¸¸èŒƒå›´\n3.å»ºè®®å®šæœŸå¤æŸ¥"
            else:
                return "æ£€æŸ¥ç»“æœæ­£å¸¸"
        elif text_category == 'normal_result':
            return "æ­£å¸¸"
        elif text_category == 'abnormal_result':
            return "å¼‚å¸¸ï¼Œéœ€è¦å…³æ³¨"
        elif text_category == 'measurement':
            return "æµ‹é‡å€¼åœ¨æ­£å¸¸èŒƒå›´å†…"
        else:
            return "æ£€æŸ¥å®Œæˆ"
    
    # é‡æ„valueåˆ—
    synthetic_df['value'] = synthetic_df.apply(reconstruct_medical_value, axis=1)
    
    # åˆ é™¤ä¸´æ—¶ç‰¹å¾åˆ—
    feature_cols = ['text_category', 'medical_term_count', 'text_complexity', 
                   'report_type', 'has_measurements', 'value_simplified',
                   'sequence_position']
    for col in feature_cols:
        if col in synthetic_df.columns:
            synthetic_df = synthetic_df.drop(columns=[col])
    
    print("âœ… æ–‡æœ¬ç»“æ„é‡æ„å®Œæˆ")
    return synthetic_df

def create_sequential_metadata(df):
    """åˆ›å»ºåºåˆ—æ•°æ®çš„å…ƒæ•°æ®"""
    print("æ­£åœ¨åˆ›å»ºåºåˆ—æ•°æ®å…ƒæ•°æ®...")
    
    # æ·»åŠ åºåˆ—ç´¢å¼•
    df['sequence_index'] = df.groupby('user_id').cumcount()
    
    # ä½¿ç”¨SingleTableMetadataæ›¿ä»£Metadata
    metadata = SingleTableMetadata()
    metadata.detect_from_dataframe(df)
    
    # è®¾ç½®user_idä¸ºåºåˆ—é”®
    metadata.set_sequence_key('user_id')
    
    # è®¾ç½®åºåˆ—ç´¢å¼•
    if 'sequence_index' in df.columns:
        metadata.update_column('sequence_index', sdtype='numerical')
        metadata.set_sequence_index('sequence_index')
        print(f"âœ“ è®¾ç½®sequence_indexä¸ºåºåˆ—ç´¢å¼•")
    else:
        # å¦‚æœæœ‰æ—¶é—´åˆ—ï¼Œè®¾ç½®ä¸ºåºåˆ—ç´¢å¼•
        time_cols = ['start_time', 'end_time', 'create_time', 'update_time']
        for col in time_cols:
            if col in df.columns:
                try:
                    metadata.set_sequence_index(col)
                    print(f"âœ“ å·²è®¾ç½® {col} ä¸ºåºåˆ—ç´¢å¼•")
                    break
                except:
                    continue
    
    print("âœ“ åºåˆ—å…ƒæ•°æ®åˆ›å»ºå®Œæˆ")
    return metadata

def main():
    import argparse
    
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description='PARåºåˆ—æ•°æ®åˆæˆæ¨¡å‹')
    parser.add_argument('--enhanced', action='store_true', 
                       help='ä½¿ç”¨å¢å¼ºæ¨¡å¼ä¿æŒJSON/æ–‡æœ¬ç»“æ„')
    parser.add_argument('--sample-size', type=int, default=3000,
                       help='é‡‡æ ·å¤§å° (é»˜è®¤: 3000)')
    parser.add_argument('--epochs', type=int, default=5,
                       help='è®­ç»ƒè½®æ•° (é»˜è®¤: 5)')
    parser.add_argument('--sequences', type=int, default=50,
                       help='ç”Ÿæˆåºåˆ—æ•°é‡ (é»˜è®¤: 50)')
    
    args = parser.parse_args()
    
    mode_desc = "å¢å¼ºæ¨¡å¼ - ä¿æŒJSON/æ–‡æœ¬ç»“æ„" if args.enhanced else "æ ‡å‡†æ¨¡å¼"
    print_model_info("PAR", f"åŸºäºæ¦‚ç‡è‡ªå›å½’çš„åºåˆ—æ•°æ®åˆæˆæ¨¡å‹ ({mode_desc})")
    
    # æ•°æ®è·¯å¾„
    data_path = "source_data/th_series_data.csv"
    output_dir = "output/par_enhanced" if args.enhanced else "output/par"
    
    try:
        # 1. åŠ è½½æ•°æ®
        df = load_data(data_path)
        
        # 2. é¢„å¤„ç†æ•°æ®
        if args.enhanced:
            # å…ˆé¢„é‡‡æ ·ä»¥ä¼˜åŒ–å†…å­˜
            df = df.sample(n=min(5000, len(df)), random_state=42)
            print(f"âœ“ é¢„é‡‡æ ·åˆ° {len(df)} æ¡è®°å½•ä»¥ä¼˜åŒ–å†…å­˜ä½¿ç”¨")
            
        df_processed, text_processor = preprocess_enhanced(
            df, sample_size=args.sample_size, enhanced_mode=args.enhanced
        )
        
        # 3. å‡†å¤‡åºåˆ—æ•°æ®
        df_sequential = prepare_sequential_data(df_processed, enhanced_mode=args.enhanced)
        if df_sequential is None:
            print("âŒ æ— æ³•å‡†å¤‡åºåˆ—æ•°æ®ï¼Œé€€å‡ºPARæ¨¡å‹è®­ç»ƒ")
            return
        
        # åˆ é™¤åŸå§‹valueåˆ—ï¼ˆå¦‚æœä½¿ç”¨å¢å¼ºæ¨¡å¼ï¼‰
        if args.enhanced and 'value' in df_sequential.columns:
            df_sequential = df_sequential.drop(columns=['value'])
        
        # 4. åˆ›å»ºåºåˆ—å…ƒæ•°æ®
        metadata = create_sequential_metadata(df_sequential)
        
        # 5. åˆ›å»º PAR åˆæˆå™¨
        print("åˆ›å»º PAR åˆæˆå™¨...")
        synthesizer = PARSynthesizer(
            metadata=metadata,
            epochs=args.epochs,
            context_columns=None,
            verbose=True
        )
        
        # 6. è®­ç»ƒå¹¶ç”Ÿæˆæ•°æ®
        print("æ­£åœ¨è®­ç»ƒPARæ¨¡å‹...")
        start_time = datetime.now()
        
        synthesizer.fit(df_sequential)
        
        train_time = datetime.now() - start_time
        print(f"âœ“ PARæ¨¡å‹è®­ç»ƒå®Œæˆï¼Œè€—æ—¶: {train_time}")
        
        # 7. ç”Ÿæˆåˆæˆæ•°æ®
        print("æ­£åœ¨ç”Ÿæˆåˆæˆåºåˆ—æ•°æ®...")
        num_sequences = min(args.sequences, len(df_sequential['user_id'].unique()))
        if args.enhanced:
            num_sequences = min(10, num_sequences)  # å¢å¼ºæ¨¡å¼ä½¿ç”¨è¾ƒå°‘åºåˆ—
            
        synthetic_data = synthesizer.sample(num_sequences=num_sequences)
        
        print(f"âœ“ æˆåŠŸç”Ÿæˆ {len(synthetic_data)} è¡Œåˆæˆåºåˆ—æ•°æ®")
        
        # 8. é‡æ„æ–‡æœ¬ç»“æ„ï¼ˆå¦‚æœä½¿ç”¨å¢å¼ºæ¨¡å¼ï¼‰
        synthetic_data_final = reconstruct_text_structure(
            synthetic_data, text_processor, enhanced_mode=args.enhanced
        )
        
        # 9. è¯„ä¼°æ¨¡å‹ï¼ˆå¢å¼ºæ¨¡å¼è·³è¿‡è¯„ä¼°ä»¥èŠ‚çœèµ„æºï¼‰
        quality_report = None
        if not args.enhanced:
            print("æ­£åœ¨è¯„ä¼°PARæ¨¡å‹...")
            try:
                quality_report = evaluate_model(
                    df_sequential, 
                    synthetic_data_final, 
                    metadata, 
                    "Enhanced_PAR" if args.enhanced else "PAR"
                )
            except Exception as e:
                print(f"âš ï¸ åºåˆ—æ•°æ®è¯„ä¼°é‡åˆ°é—®é¢˜: {e}")
        
        # 10. ä¿å­˜ç»“æœ
        save_results(
            synthetic_data_final, 
            "Enhanced_PAR" if args.enhanced else "PAR", 
            output_dir, 
            metadata, 
            quality_report, 
            train_time
        )
        
        # 11. æ‰“å°è¯¦ç»†ç»Ÿè®¡
        print("\n=== åºåˆ—æ•°æ®ç»Ÿè®¡å¯¹æ¯” ===")
        print(f"\nåŸå§‹æ•°æ®:")
        print(f"- æ€»è®°å½•æ•°: {len(df_sequential)}")
        print(f"- ç”¨æˆ·æ•°é‡: {df_sequential['user_id'].nunique()}")
        print(f"- å¹³å‡æ¯ç”¨æˆ·è®°å½•æ•°: {len(df_sequential) / df_sequential['user_id'].nunique():.1f}")
        
        print(f"\nåˆæˆæ•°æ®:")
        print(f"- æ€»è®°å½•æ•°: {len(synthetic_data_final)}")
        print(f"- ç”¨æˆ·æ•°é‡: {synthetic_data_final['user_id'].nunique()}")
        print(f"- å¹³å‡æ¯ç”¨æˆ·è®°å½•æ•°: {len(synthetic_data_final) / synthetic_data_final['user_id'].nunique():.1f}")
        
        if args.enhanced:
            print("\nğŸ“Š å¢å¼ºæ¨¡å¼ç»“æ„å¯¹æ¯”:")
            print("åŸå§‹æ•°æ®æ ·ä¾‹:")
            original_sample = df[['user_id', 'indicator', 'value']].head(3)
            print(original_sample.to_string(index=False))
            
            print("\nå¢å¼ºPARç”Ÿæˆæ•°æ®æ ·ä¾‹:")
            enhanced_sample = synthetic_data_final[['user_id', 'indicator', 'value']].head(3)
            print(enhanced_sample.to_string(index=False))
        
        print(f"\nâœ… PAR æ¨¡å‹æ‰§è¡Œå®Œæˆï¼")
        print(f"ç»“æœä¿å­˜åœ¨: {output_dir}")
        
    except Exception as e:
        print(f"âŒ æ‰§è¡Œè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 