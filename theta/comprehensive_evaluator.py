#!/usr/bin/env python3
"""
ç»¼åˆæ•°æ®è¯„æµ‹å™¨ - ä¸“æ³¨äºåˆ†æå’ŒæŠ¥å‘Šç”Ÿæˆ
åŠŸèƒ½ï¼š
- è¯»å–å·²ç”Ÿæˆçš„åˆæˆæ•°æ®
- ç»Ÿè®¡åˆ†æä¸åŸæ•°æ®å¯¹æ¯”
- ç”Ÿæˆåˆ†æå›¾è¡¨
- ç”Ÿæˆå®Œæ•´åˆ†ææŠ¥å‘Š
æ³¨æ„ï¼šæ¨¡å‹è®­ç»ƒå’Œæ•°æ®ç”Ÿæˆè¯·ä½¿ç”¨ run_all_models.py
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import sys
import os
import glob
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®matplotlibåç«¯å’Œæ ·å¼
import matplotlib
matplotlib.use('Agg')  # ä½¿ç”¨éäº¤äº’å¼åç«¯
plt.style.use('default')

class ComprehensiveEvaluator:
    def __init__(self, data_path='source_data/th_series_data.csv'):
        """åˆå§‹åŒ–è¯„æµ‹å™¨"""
        self.data_path = data_path
        self.original_data = None
        self.user_data = None
        self.synthetic_datasets = {}  # å­˜å‚¨å¤šä¸ªæ¨¡å‹çš„åˆæˆæ•°æ®
        self.user_id = None
        self.evaluation_results = {}
        
        # æ”¯æŒçš„æ¨¡å‹åˆ—è¡¨
        self.supported_models = [
            'gaussian_copula',
            'ctgan', 
            'copulagan',
            'tvae',
            'par'
        ]
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs('output/graph', exist_ok=True)
        os.makedirs('output/comprehensive_reports', exist_ok=True)
    
    def load_data(self):
        """åŠ è½½åŸå§‹æ•°æ®"""
        print("ğŸ“‚ åŠ è½½åŸå§‹æ•°æ®...")
        self.original_data = pd.read_csv(self.data_path)
        print(f"âœ“ æ•°æ®åŠ è½½å®Œæˆ: {self.original_data.shape}")
        return True
    
    def analyze_users(self, min_records=200, max_records=5000):
        """åˆ†æå¯ç”¨ç”¨æˆ·"""
        print(f"\nğŸ‘¥ åˆ†æå¯ç”¨ç”¨æˆ· (è®°å½•æ•°èŒƒå›´: {min_records}-{max_records})...")
        
        user_counts = self.original_data['user_id'].value_counts()
        suitable_users = user_counts[(user_counts >= min_records) & (user_counts <= max_records)]
        
        print(f"ğŸ“Š ç¬¦åˆæ¡ä»¶çš„ç”¨æˆ·: {len(suitable_users)}ä¸ª")
        
        if len(suitable_users) == 0:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°åˆé€‚çš„ç”¨æˆ·")
            return None
        
        # æ˜¾ç¤ºæ¨èç”¨æˆ·
        print("\næ¨èç”¨æˆ·:")
        for i, (user_id, count) in enumerate(suitable_users.head(10).items(), 1):
            user_data = self.original_data[self.original_data['user_id'] == user_id]
            indicators = user_data['indicator'].nunique()
            
            try:
                dates = pd.to_datetime(user_data['start_time'], errors='coerce').dropna()
                date_range = (dates.max() - dates.min()).days if len(dates) > 0 else 0
            except:
                date_range = 0
            
            print(f"  {i:2d}. ç”¨æˆ·{user_id}: {count}æ¡è®°å½•, {indicators}ç§æŒ‡æ ‡, {date_range}å¤©")
        
        return suitable_users
    
    def select_user(self, user_id, max_records=None):
        """é€‰æ‹©å¹¶å‡†å¤‡ç”¨æˆ·æ•°æ®"""
        print(f"\nğŸ¯ é€‰æ‹©ç”¨æˆ· {user_id}...")
        
        if user_id not in self.original_data['user_id'].values:
            print(f"âŒ ç”¨æˆ· {user_id} ä¸å­˜åœ¨")
            return False
        
        self.user_id = user_id
        
        # æå–ç”¨æˆ·æ•°æ®
        self.user_data = self.original_data[self.original_data['user_id'] == user_id].copy()
        print(f"âœ“ åŸå§‹æ•°æ®: {len(self.user_data)} æ¡è®°å½•")
        
        # æ•°æ®é‡‡æ ·ï¼ˆå¦‚æœæŒ‡å®šäº†max_recordsï¼‰
        if max_records and len(self.user_data) > max_records:
            self.user_data = self.user_data.sample(n=max_records, random_state=42)
            print(f"âœ“ é‡‡æ ·è‡³: {len(self.user_data)} æ¡è®°å½•")
        else:
            print(f"âœ“ ä½¿ç”¨å…¨éƒ¨æ•°æ®: {len(self.user_data)} æ¡è®°å½•")
        
        # æ•°æ®é¢„å¤„ç†
        self._preprocess_user_data()
        
        return True
    
    def _preprocess_user_data(self):
        """é¢„å¤„ç†ç”¨æˆ·æ•°æ®"""
        print("ğŸ”§ é¢„å¤„ç†ç”¨æˆ·æ•°æ®...")
        
        # åˆ é™¤ä¸éœ€è¦çš„åˆ—
        columns_to_drop = ['id', 'source_table_id', 'comment', 'indicator_id', 'user_id']
        for col in columns_to_drop:
            if col in self.user_data.columns:
                self.user_data = self.user_data.drop(columns=[col])
        
        # å¤„ç†æ—¶é—´ - ä¼˜å…ˆä½¿ç”¨start_timeï¼Œå¦‚æœæ— æ•ˆåˆ™ä½¿ç”¨create_time
        time_column = None
        if 'start_time' in self.user_data.columns:
            try:
                start_time = pd.to_datetime(self.user_data['start_time'], errors='coerce')
                valid_start_time = start_time.notna().sum()
                
                if valid_start_time > len(self.user_data) * 0.5:  # å¦‚æœè¶…è¿‡50%çš„æ•°æ®æœ‰æ•ˆ
                    time_column = 'start_time'
                    self.user_data['start_time'] = start_time
                    self.user_data = self.user_data.dropna(subset=['start_time'])
                    
                    # æŒ‰æ—¶é—´æ’åºå¹¶åˆ›å»ºè¿ç»­æ—¥æœŸæ—¶é—´åºåˆ—
                    self.user_data = self.user_data.sort_values('start_time').reset_index(drop=True)
                    self.last_date = self.user_data['start_time'].max()
                    
                    # åˆ›å»ºè¿ç»­çš„æ—¥æœŸæ—¶é—´åºåˆ—
                    base_time = self.user_data['start_time'].min()
                    self.user_data['sequence_datetime'] = pd.to_datetime([
                        base_time + pd.Timedelta(hours=i) for i in range(len(self.user_data))
                    ])
                    
                    self.user_data['hour'] = self.user_data['start_time'].dt.hour
                    
                    # ä¿ç•™åŸå§‹æ—¶é—´ç”¨äºå‚è€ƒ
                    self.user_data['original_start_time'] = self.user_data['start_time']
                    self.user_data = self.user_data.drop(columns=['start_time'])
                    print(f"âœ“ ä½¿ç”¨start_timeè¿›è¡Œæ—¶é—´å¤„ç†ï¼Œåˆ›å»ºdatetimeåºåˆ— {self.user_data['sequence_datetime'].min()} åˆ° {self.user_data['sequence_datetime'].max()}ï¼Œæœ€åæ—¥æœŸ: {self.last_date.date()}")
                else:
                    print(f"âš ï¸ start_timeæœ‰æ•ˆç‡è¾ƒä½ ({valid_start_time}/{len(self.user_data)})ï¼Œå°è¯•ä½¿ç”¨create_time")
                    time_column = 'create_time'
                    
            except Exception as e:
                print(f"âš ï¸ start_timeå¤„ç†å¤±è´¥: {e}ï¼Œå°è¯•ä½¿ç”¨create_time")
                time_column = 'create_time'
        
        # å¦‚æœstart_timeä¸å¯ç”¨ï¼Œä½¿ç”¨create_time
        if time_column != 'start_time' and 'create_time' in self.user_data.columns:
            try:
                self.user_data['create_time'] = pd.to_datetime(self.user_data['create_time'], errors='coerce')
                valid_create_time = self.user_data['create_time'].notna().sum()
                
                if valid_create_time > 0:
                    self.user_data = self.user_data.dropna(subset=['create_time'])
                    self.last_date = self.user_data['create_time'].max()
                    
                    # å¯¹äºåºåˆ—æ¨¡å‹ï¼Œåˆ›å»ºåŸºäºæ—¶é—´çš„è¿ç»­åºåˆ—
                    # æŒ‰create_timeæ’åº
                    self.user_data = self.user_data.sort_values('create_time').reset_index(drop=True)
                    
                    # åˆ›å»ºè¿ç»­çš„æ—¥æœŸæ—¶é—´åºåˆ—ï¼ˆæ¯æ¡è®°å½•é—´éš”1å°æ—¶ï¼‰
                    base_time = self.user_data['create_time'].min()
                    self.user_data['sequence_datetime'] = pd.to_datetime([
                        base_time + pd.Timedelta(hours=i) for i in range(len(self.user_data))
                    ])
                    
                    self.user_data['hour'] = self.user_data['create_time'].dt.hour
                    
                    # ä¿ç•™åŸå§‹create_timeç”¨äºå‚è€ƒ
                    self.user_data['original_create_time'] = self.user_data['create_time']
                    self.user_data = self.user_data.drop(columns=['create_time'])
                    
                    print(f"âœ“ ä½¿ç”¨create_timeè¿›è¡Œæ—¶é—´å¤„ç†ï¼Œåˆ›å»ºdatetimeåºåˆ— {self.user_data['sequence_datetime'].min()} åˆ° {self.user_data['sequence_datetime'].max()}ï¼Œæœ€åæ—¥æœŸ: {self.last_date.date()}")
                else:
                    print(f"âš ï¸ create_timeä¹Ÿæ— æ•ˆï¼Œä½¿ç”¨å½“å‰æ—¶é—´")
                    self.last_date = datetime.now()
                    
            except Exception as e:
                print(f"âš ï¸ create_timeå¤„ç†å¤±è´¥: {e}")
                self.last_date = datetime.now()
        
        if time_column is None:
            self.last_date = datetime.now()
        
        # å¤„ç†valueåˆ— - åº”ç”¨æ™ºèƒ½å¤„ç†é€»è¾‘
        if 'value' in self.user_data.columns:
            # åº”ç”¨ä¸è®­ç»ƒæ•°æ®ç›¸åŒçš„æ™ºèƒ½valueå¤„ç†
            self.user_data = self._smart_process_value_field(self.user_data)
            print(f"âœ“ valueæ™ºèƒ½å¤„ç†å®Œæˆ")
        
        # å¤„ç†indicatoråˆ— - ä¸ºäº†ä¸è®­ç»ƒæ•°æ®ä¿æŒä¸€è‡´ï¼Œä¸è¿›è¡Œ'other'æ ‡è®°
        if 'indicator' in self.user_data.columns:
            # ä¿æŒåŸå§‹æŒ‡æ ‡ï¼Œä¸è¿›è¡Œ'other'æ ‡è®°ä»¥ä¸è®­ç»ƒæ•°æ®ä¸€è‡´
            indicator_count = self.user_data['indicator'].nunique()
            print(f"âœ“ ä¿æŒåŸå§‹æŒ‡æ ‡ (å…±{indicator_count}ä¸ª)ï¼Œä¸è®­ç»ƒæ•°æ®ä¿æŒä¸€è‡´")
        
        # æ’åº
        if 'day_of_year' in self.user_data.columns:
            self.user_data = self.user_data.sort_values('day_of_year').reset_index(drop=True)
        
        print(f"âœ“ é¢„å¤„ç†å®Œæˆ: {self.user_data.shape}")
    
    def load_synthetic_data(self, models=None):
        """åŠ è½½å·²ç”Ÿæˆçš„åˆæˆæ•°æ®"""
        print(f"\nğŸ“‚ åŠ è½½åˆæˆæ•°æ®...")
        
        if models is None:
            models = self.supported_models
        
        loaded_models = []
        
        for model in models:
            # æ„å»ºæ•°æ®æ–‡ä»¶è·¯å¾„ - æ”¯æŒå¤šç§æ–‡ä»¶å‘½åæ ¼å¼
            data_files = [
                f"output/{model}/{model.upper()}_synthetic_data.csv",
                f"output/{model}/{model.title()}_synthetic_data.csv", 
                f"output/{model}/Enhanced_{model.upper()}_synthetic_data.csv",
                f"output/{model}/synthetic_data.csv"
            ]
            
            # é’ˆå¯¹ç‰¹æ®Šæƒ…å†µçš„é¢å¤–æ–‡ä»¶å
            if model == 'gaussian_copula':
                data_files.extend([
                    "output/gaussian_copula/GaussianCopula_synthetic_data.csv",
                    "output/gaussian_copula/Gaussian_Copula_synthetic_data.csv"
                ])
            elif model == 'par':
                data_files.extend([
                    "output/par_enhanced/Enhanced_PAR_synthetic_data.csv",
                    "output/par/Enhanced_PAR_synthetic_data.csv"
                ])
            
            # å°è¯•å¤šç§å¯èƒ½çš„æ–‡ä»¶å
            loaded = False
            for data_file in data_files:
                if os.path.exists(data_file):
                    try:
                        synthetic_data = pd.read_csv(data_file)
                        
                        # è¿‡æ»¤ç”¨æˆ·æ•°æ®ï¼ˆå¦‚æœåŒ…å«user_idåˆ—ï¼‰
                        if 'user_id' in synthetic_data.columns and self.user_id:
                            user_synthetic = synthetic_data[synthetic_data['user_id'] == self.user_id]
                            if len(user_synthetic) > 0:
                                synthetic_data = user_synthetic
                            # å¦‚æœæ²¡æœ‰ç”¨æˆ·æ•°æ®ï¼Œä½¿ç”¨å…¨éƒ¨æ•°æ®
                        
                        # å¤„ç†æ—¶é—´åˆ—ï¼šå¦‚æœæ²¡æœ‰houråˆ—ä½†æœ‰start_time_houråˆ—ï¼Œåˆ™åˆ›å»ºhouråˆ—
                        if 'hour' not in synthetic_data.columns and 'start_time_hour' in synthetic_data.columns:
                            synthetic_data['hour'] = synthetic_data['start_time_hour']
                        
                        self.synthetic_datasets[model] = synthetic_data
                        
                        print(f"âœ… {model}: åŠ è½½ {len(self.synthetic_datasets[model])} æ¡æ•°æ®")
                        loaded_models.append(model)
                        loaded = True
                        break
                        
                    except Exception as e:
                        print(f"âš ï¸ {model}: è¯»å–æ–‡ä»¶å¤±è´¥ - {e}")
                        continue
            
            if not loaded:
                print(f"âŒ {model}: æœªæ‰¾åˆ°åˆæˆæ•°æ®æ–‡ä»¶")
                # å°è¯•æœç´¢è¾“å‡ºç›®å½•
                search_pattern = f"output/{model}/*synthetic*.csv"
                found_files = glob.glob(search_pattern)
                if found_files:
                    print(f"   ğŸ’¡ å‘ç°æ–‡ä»¶: {found_files}")
        
        if not loaded_models:
            print("âŒ æœªæ‰¾åˆ°ä»»ä½•åˆæˆæ•°æ®æ–‡ä»¶")
            print("ğŸ’¡ è¯·å…ˆè¿è¡Œ python3 run_all_models.py ç”Ÿæˆåˆæˆæ•°æ®")
            return False
        
        print(f"âœ… æˆåŠŸåŠ è½½ {len(loaded_models)} ä¸ªæ¨¡å‹çš„åˆæˆæ•°æ®: {', '.join(loaded_models)}")
        return loaded_models
    
    def statistical_analysis(self):
        """ç»Ÿè®¡åˆ†æ - æ”¯æŒå¤šæ¨¡å‹å¯¹æ¯”"""
        print(f"\nğŸ“Š è¿›è¡Œç»Ÿè®¡åˆ†æ...")
        
        if not self.synthetic_datasets:
            print("âŒ æ²¡æœ‰åˆæˆæ•°æ®å¯ä¾›åˆ†æ")
            return {}
        
        results = {}
        
        # åŸºæœ¬ç»Ÿè®¡
        original_records = len(self.user_data)
        original_indicators = self.user_data['indicator'].nunique() if 'indicator' in self.user_data.columns else 0
        
        results['basic_stats'] = {
            'original_records': original_records,
            'original_indicators': original_indicators,
        }
        
        # ä¸ºæ¯ä¸ªæ¨¡å‹æ·»åŠ ç»Ÿè®¡
        for model_name, synthetic_data in self.synthetic_datasets.items():
            results['basic_stats'][f'{model_name}_records'] = len(synthetic_data)
            results['basic_stats'][f'{model_name}_indicators'] = synthetic_data['indicator'].nunique() if 'indicator' in synthetic_data.columns else 0
        
        # æŒ‡æ ‡åˆ†å¸ƒå¯¹æ¯” - æ”¯æŒå¤šæ¨¡å‹
        if 'indicator' in self.user_data.columns:
            orig_indicators = self.user_data['indicator'].value_counts()
            
            results['indicator_analysis'] = {
                'original_top5': orig_indicators.head().to_dict(),
                'models': {}
            }
            
            # è¯¦ç»†åˆ†æå‰16ä¸ªæœ€å¸¸è§æŒ‡æ ‡
            top16_indicators = orig_indicators.head(16)
            
            # ä¸ºæ¯ä¸ªæ¨¡å‹åˆ†ææŒ‡æ ‡
            for model_name, synthetic_data in self.synthetic_datasets.items():
                if 'indicator' not in synthetic_data.columns:
                    continue
                    
                synt_indicators = synthetic_data['indicator'].value_counts()
                
                # å‰16ä¸ªæŒ‡æ ‡çš„è¯¦ç»†åˆ†æ
                top16_analysis = {}
                for indicator in top16_indicators.index:
                    orig_count = orig_indicators.get(indicator, 0)
                    synt_count = synt_indicators.get(indicator, 0)
                    
                    # è®¡ç®—è¯¥æŒ‡æ ‡çš„æ•°å€¼åˆ†å¸ƒç›¸ä¼¼æ€§
                    indicator_similarity = self._analyze_indicator_values(indicator, orig_count, synt_count, synthetic_data)
                    
                    top16_analysis[indicator] = {
                        'original_count': orig_count,
                        'synthetic_count': synt_count,
                        'count_similarity': 1 - abs(orig_count - synt_count) / max(orig_count, synt_count, 1),
                        'value_analysis': indicator_similarity
                    }
                
                results['indicator_analysis']['models'][model_name] = {
                    'synthetic_all_indicators': synt_indicators.to_dict(),  # å®Œæ•´æŒ‡æ ‡ç»Ÿè®¡ï¼Œç”¨äºå›¾è¡¨ç”Ÿæˆ
                    'synthetic_top10': synt_indicators.head(10).to_dict(),  # æ”¹ä¸ºå‰10ä¸ªæŒ‡æ ‡
                    'synthetic_top5': synt_indicators.head(5).to_dict(),   # ä¿æŒå‘åå…¼å®¹
                    'coverage_similarity': self._calculate_coverage_similarity(orig_indicators, synt_indicators),
                    'top16_detailed_analysis': top16_analysis
                }
        
        # æ•°å€¼åˆ†å¸ƒå¯¹æ¯” - æ”¯æŒå¤šæ¨¡å‹
        if 'value' in self.user_data.columns:
            try:
                orig_values = pd.to_numeric(self.user_data['value'], errors='coerce').dropna()
                
                if len(orig_values) > 0:
                    results['value_analysis'] = {
                        'original_stats': orig_values.describe().to_dict(),
                        'models': {}
                    }
                    
                    for model_name, synthetic_data in self.synthetic_datasets.items():
                        if 'value' in synthetic_data.columns:
                            synt_values = pd.to_numeric(synthetic_data['value'], errors='coerce').dropna()
                            
                            if len(synt_values) > 0:
                                results['value_analysis']['models'][model_name] = {
                                    'synthetic_stats': synt_values.describe().to_dict(),
                                    'distribution_similarity': self._calculate_distribution_similarity(orig_values, synt_values)
                                }
            except:
                results['value_analysis'] = {'error': 'Failed to analyze numeric values'}
        
        # æ—¶é—´åˆ†å¸ƒå¯¹æ¯” - æ”¯æŒå¤šæ¨¡å‹
        if 'hour' in self.user_data.columns:
            orig_hours = self.user_data['hour'].value_counts().sort_index()
            
            results['time_analysis'] = {
                'original_hour_dist': orig_hours.to_dict(),
                'models': {}
            }
            
            for model_name, synthetic_data in self.synthetic_datasets.items():
                if 'hour' in synthetic_data.columns:
                    synt_hours = synthetic_data['hour'].value_counts().sort_index()
                    
                    results['time_analysis']['models'][model_name] = {
                        'synthetic_hour_dist': synt_hours.to_dict(),
                        'peak_hours_similarity': self._calculate_peak_similarity(orig_hours, synt_hours)
                    }
        
        self.evaluation_results = results
        print(f"âœ… ç»Ÿè®¡åˆ†æå®Œæˆ")
        
        return results
    
    def _smart_process_value_field(self, df):
        """æ ¹æ®indicatorç±»å‹æ™ºèƒ½å¤„ç†valueå­—æ®µï¼ˆä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´ï¼‰"""
        if 'indicator' not in df.columns or 'value' not in df.columns:
            return df
        
        print("ğŸ“Š æ ¹æ®indicatorç±»å‹æ™ºèƒ½å¤„ç†valueå­—æ®µ...")
        
        # å…ˆå°è¯•è½¬æ¢ä¸ºæ•°å€¼
        df['value'] = pd.to_numeric(df['value'], errors='coerce')
        
        # å¤„ç†JSONæ ¼å¼çš„value
        mask = df['value'].isna()
        if mask.any():
            for idx in df.index[mask]:
                original_value = str(df.at[idx, 'value'])
                if 'primary_value' in original_value:
                    try:
                        import json
                        value_dict = json.loads(original_value.replace("'", '"'))
                        if 'primary_value' in value_dict:
                            df.at[idx, 'value'] = float(value_dict['primary_value'])
                    except:
                        df.at[idx, 'value'] = 0.0
                else:
                    df.at[idx, 'value'] = 0.0
        
        # æ ¹æ®indicatorç±»å‹è¿›è¡Œç‰¹æ®Šå¤„ç†
        timestamp_count = 0
        for idx in df.index:
            indicator = str(df.at[idx, 'indicator']).lower()
            value = df.at[idx, 'value']
            
            if pd.isna(value):
                continue
                
            # 1. æ—¶é—´ç›¸å…³æŒ‡æ ‡ç»Ÿä¸€å¤„ç†
            if any(keyword in indicator for keyword in ['_start_time', '_end_time', 'sleep_start', 'sleep_end', '_time_', 'time_avg', 'avg_start', 'avg_end']):
                if value > 1000000000:  # Unixæ—¶é—´æˆ³
                    try:
                        import datetime
                        dt = datetime.datetime.fromtimestamp(value)
                        df.at[idx, 'value'] = dt.hour  # è½¬æ¢ä¸ºå°æ—¶ (0-23)
                        timestamp_count += 1
                        if timestamp_count <= 10:  # åªæ˜¾ç¤ºå‰10ä¸ªè½¬æ¢
                            print(f"ğŸ“Š   æ—¶é—´æˆ³è½¬æ¢: {indicator} {value} â†’ å°æ—¶{dt.hour}")
                    except:
                        df.at[idx, 'value'] = 12  # é»˜è®¤ä¸­åˆ12ç‚¹
                elif value > 86400:  # è¶…è¿‡24å°æ—¶çš„ç§’æ•°ï¼Œè½¬æ¢ä¸ºå°æ—¶
                    df.at[idx, 'value'] = (value % 86400) / 3600  # è½¬æ¢ä¸ºå°æ—¶
                elif value > 24:  # å¯èƒ½æ˜¯å°æ—¶ä½†è¶…å‡ºèŒƒå›´
                    df.at[idx, 'value'] = value % 24
                elif value < 0:
                    df.at[idx, 'value'] = 0
            
            # 2. è¡€æ°§ç›¸å…³ï¼Œé™åˆ¶åœ¨åˆç†èŒƒå›´ (90-100)
            elif 'blood_oxygen' in indicator or 'oxygen' in indicator:
                if value > 100:
                    df.at[idx, 'value'] = min(100, max(90, value % 100 + 90))
                elif value < 50:
                    df.at[idx, 'value'] = max(90, 95 + (value % 10))
            
            # 3. å¿ƒç‡ç›¸å…³ï¼Œé™åˆ¶åœ¨åˆç†èŒƒå›´ (40-200)
            elif 'heart_rate' in indicator or 'hr_' in indicator:
                if value > 200:
                    df.at[idx, 'value'] = min(200, max(40, value % 160 + 40))
                elif value < 30:
                    df.at[idx, 'value'] = max(40, 60 + (value % 20))
            
            # 4. ç™¾åˆ†æ¯”ç±»å‹ï¼Œé™åˆ¶åœ¨0-100
            elif any(keyword in indicator for keyword in ['percentage', 'percent', 'ratio']):
                if value > 100:
                    df.at[idx, 'value'] = value % 100
                elif value < 0:
                    df.at[idx, 'value'] = abs(value) % 100
            
            # 5. æ­¥æ•°ç›¸å…³ï¼Œé™åˆ¶åœ¨åˆç†èŒƒå›´ (0-50000)
            elif 'steps' in indicator:
                if value > 50000:
                    df.at[idx, 'value'] = value % 50000
                elif value < 0:
                    df.at[idx, 'value'] = abs(value) % 50000
            
            # 6. æ—¶é•¿ç±»å‹(ç§’)ï¼Œè½¬æ¢ä¸ºåˆ†é’Ÿå¹¶é™åˆ¶åˆç†èŒƒå›´
            elif 'duration' in indicator:
                if value > 86400:  # è¶…è¿‡24å°æ—¶çš„ç§’æ•°
                    df.at[idx, 'value'] = min(1440, (value % 86400) / 60)  # è½¬æ¢ä¸ºåˆ†é’Ÿï¼Œæœ€å¤§24å°æ—¶
                elif value > 3600:  # è¶…è¿‡1å°æ—¶
                    df.at[idx, 'value'] = min(1440, value / 60)  # è½¬æ¢ä¸ºåˆ†é’Ÿï¼Œæœ€å¤§24å°æ—¶
                elif value > 1440:  # å¦‚æœå·²ç»æ˜¯åˆ†é’Ÿä½†è¶…è¿‡24å°æ—¶
                    df.at[idx, 'value'] = value % 1440  # é™åˆ¶åœ¨24å°æ—¶å†…
        
        if timestamp_count > 0:
            print(f"ğŸ“Š   å…±è½¬æ¢äº† {timestamp_count} ä¸ªæ—¶é—´æˆ³")
        
        return df
    
    def _calculate_coverage_similarity(self, orig_dist, synt_dist):
        """è®¡ç®—è¦†ç›–ç‡ç›¸ä¼¼æ€§"""
        common_items = set(orig_dist.index) & set(synt_dist.index)
        total_items = set(orig_dist.index) | set(synt_dist.index)
        return len(common_items) / len(total_items) if total_items else 0
    
    def _calculate_distribution_similarity(self, orig_values, synt_values):
        """è®¡ç®—åˆ†å¸ƒç›¸ä¼¼æ€§"""
        try:
            # ä½¿ç”¨KSæ£€éªŒç»Ÿè®¡é‡çš„ç›¸ä¼¼æ€§
            from scipy import stats
            ks_stat, p_value = stats.ks_2samp(orig_values, synt_values)
            return 1 - ks_stat  # è½¬æ¢ä¸ºç›¸ä¼¼æ€§åˆ†æ•°
        except:
            # å¦‚æœscipyä¸å¯ç”¨ï¼Œä½¿ç”¨ç®€å•çš„ç»Ÿè®¡é‡å¯¹æ¯”
            orig_mean, orig_std = orig_values.mean(), orig_values.std()
            synt_mean, synt_std = synt_values.mean(), synt_values.std()
            
            mean_diff = abs(orig_mean - synt_mean) / max(orig_mean, synt_mean, 1)
            std_diff = abs(orig_std - synt_std) / max(orig_std, synt_std, 1)
            
            return 1 - (mean_diff + std_diff) / 2
    
    def _calculate_peak_similarity(self, orig_hours, synt_hours):
        """è®¡ç®—å³°å€¼æ—¶é—´ç›¸ä¼¼æ€§"""
        orig_peak = orig_hours.idxmax()
        synt_peak = synt_hours.idxmax()
        
        # è®¡ç®—å³°å€¼æ—¶é—´å·®å¼‚
        time_diff = abs(orig_peak - synt_peak)
        time_diff = min(time_diff, 24 - time_diff)  # è€ƒè™‘å¾ªç¯æ€§
        
        return 1 - time_diff / 12  # æ ‡å‡†åŒ–åˆ°0-1
    
    def _analyze_indicator_values(self, indicator, orig_count, synt_count, synthetic_data):
        """åˆ†æç‰¹å®šæŒ‡æ ‡çš„æ•°å€¼åˆ†å¸ƒ"""
        try:
            # æå–è¯¥æŒ‡æ ‡çš„åŸå§‹æ•°æ®å€¼
            orig_indicator_data = self.user_data[self.user_data['indicator'] == indicator]['value']
            synt_indicator_data = synthetic_data[synthetic_data['indicator'] == indicator]['value']
            
            if len(orig_indicator_data) == 0 or len(synt_indicator_data) == 0:
                return {'error': 'No data for this indicator'}
            
            # å°è¯•æ•°å€¼åŒ–åˆ†æ
            orig_numeric = pd.to_numeric(orig_indicator_data, errors='coerce')
            synt_numeric = pd.to_numeric(synt_indicator_data, errors='coerce')
            
            orig_numeric_valid = orig_numeric.dropna()
            synt_numeric_valid = synt_numeric.dropna()
            
            analysis = {}
            
            # å¦‚æœæ•°å€¼åŒ–æˆåŠŸç‡é«˜ï¼Œè¿›è¡Œæ•°å€¼åˆ†æ
            if len(orig_numeric_valid) > len(orig_indicator_data) * 0.7 and len(synt_numeric_valid) > len(synt_indicator_data) * 0.7:
                analysis['type'] = 'numerical'
                analysis['original_stats'] = {
                    'mean': float(orig_numeric_valid.mean()),
                    'std': float(orig_numeric_valid.std()),
                    'min': float(orig_numeric_valid.min()),
                    'max': float(orig_numeric_valid.max()),
                    'count': len(orig_numeric_valid)
                }
                analysis['synthetic_stats'] = {
                    'mean': float(synt_numeric_valid.mean()),
                    'std': float(synt_numeric_valid.std()),
                    'min': float(synt_numeric_valid.min()),
                    'max': float(synt_numeric_valid.max()),
                    'count': len(synt_numeric_valid)
                }
                
                # è®¡ç®—æ•°å€¼åˆ†å¸ƒç›¸ä¼¼æ€§
                if len(orig_numeric_valid) > 0 and len(synt_numeric_valid) > 0:
                    analysis['distribution_similarity'] = self._calculate_distribution_similarity(orig_numeric_valid, synt_numeric_valid)
                else:
                    analysis['distribution_similarity'] = 0.0
            else:
                # åˆ†ç±»æ•°æ®åˆ†æ
                analysis['type'] = 'categorical'
                orig_values = orig_indicator_data.value_counts()
                synt_values = synt_indicator_data.value_counts()
                
                analysis['original_top_values'] = orig_values.head(5).to_dict()
                analysis['synthetic_top_values'] = synt_values.head(5).to_dict()
                analysis['value_similarity'] = self._calculate_coverage_similarity(orig_values, synt_values)
            
            return analysis
            
        except Exception as e:
            return {'error': f'Analysis failed: {str(e)}'}
    
    def _generate_top16_indicators_chart(self):
        """ç”Ÿæˆå‰16ä¸ªæŒ‡æ ‡çš„è¯¦ç»†åˆ†æå›¾è¡¨ - å¤šæ¨¡å‹æ”¯æŒ"""
        print(f"ğŸ“Š ç”Ÿæˆå‰16ä¸ªæŒ‡æ ‡è¯¦ç»†åˆ†æå›¾è¡¨...")
        
        try:
            indicator_analysis = self.evaluation_results.get('indicator_analysis', {})
            if 'models' not in indicator_analysis or not indicator_analysis['models']:
                print("âš ï¸ å‰16ä¸ªæŒ‡æ ‡åˆ†ææ•°æ®ä¸å¯ç”¨")
                return None
            
            # é€‰æ‹©ç¬¬ä¸€ä¸ªå¯ç”¨æ¨¡å‹è¿›è¡Œè¯¦ç»†å±•ç¤º
            first_model = list(indicator_analysis['models'].keys())[0]
            top16_data = indicator_analysis['models'][first_model]['top16_detailed_analysis']
            synthetic_data = self.synthetic_datasets[first_model]
            
            # åˆ›å»º4x4çš„å­å›¾
            fig, axes = plt.subplots(4, 4, figsize=(24, 20))
            fig.suptitle(f'User {self.user_id} - Top 16 Indicators Detailed Analysis', fontsize=18, fontweight='bold')
            
            # è·å–æ‰€æœ‰æŒ‡æ ‡åç§°
            indicators = list(top16_data.keys())[:16]  # ç¡®ä¿æœ€å¤š16ä¸ª
            
            for i, indicator in enumerate(indicators):
                row = i // 4
                col = i % 4
                ax = axes[row, col]
                
                indicator_info = top16_data[indicator]
                value_analysis = indicator_info.get('value_analysis', {})
                
                # æ ¹æ®æ•°æ®ç±»å‹é€‰æ‹©ä¸åŒçš„å¯è§†åŒ–æ–¹å¼
                if value_analysis.get('type') == 'numerical':
                    # æ•°å€¼å‹æ•°æ®ï¼šç›´æ–¹å›¾å¯¹æ¯”
                    self._plot_numerical_indicator(ax, indicator, value_analysis, synthetic_data)
                elif value_analysis.get('type') == 'categorical':
                    # åˆ†ç±»å‹æ•°æ®ï¼šæ¡å½¢å›¾å¯¹æ¯”
                    self._plot_categorical_indicator(ax, indicator, value_analysis, synthetic_data)
                else:
                    # å¦‚æœåˆ†æå¤±è´¥ï¼Œæ˜¾ç¤ºåŸºæœ¬ç»Ÿè®¡
                    self._plot_basic_indicator_stats(ax, indicator, indicator_info)
                
                # è®¾ç½®æ ‡é¢˜ï¼ˆæˆªæ–­è¿‡é•¿çš„æŒ‡æ ‡åï¼‰
                title = indicator if len(indicator) <= 15 else indicator[:12] + "..."
                ax.set_title(title, fontsize=10, fontweight='bold')
            
            # éšè—å¤šä½™çš„å­å›¾
            for i in range(len(indicators), 16):
                row = i // 4
                col = i % 4
                axes[row, col].set_visible(False)
            
            plt.tight_layout()
            
            # ä¿å­˜å›¾è¡¨
            top16_chart_file = f'output/graph/user_{self.user_id}_top16_indicators_analysis.png'
            plt.savefig(top16_chart_file, dpi=300, bbox_inches='tight')
            plt.close()
            
            print(f"âœ… å‰16ä¸ªæŒ‡æ ‡åˆ†æå›¾è¡¨å·²ä¿å­˜: {top16_chart_file}")
            return top16_chart_file
            
        except Exception as e:
            print(f"âš ï¸ å‰16ä¸ªæŒ‡æ ‡å›¾è¡¨ç”Ÿæˆå¤±è´¥: {e}")
            return None
    
    def _plot_numerical_indicator(self, ax, indicator, value_analysis, synthetic_data):
        """ç»˜åˆ¶æ•°å€¼å‹æŒ‡æ ‡çš„åˆ†å¸ƒå¯¹æ¯”"""
        try:
            # æå–è¯¥æŒ‡æ ‡çš„æ•°æ®
            orig_data = self.user_data[self.user_data['indicator'] == indicator]['value']
            synt_data = synthetic_data[synthetic_data['indicator'] == indicator]['value']
            
            orig_numeric = pd.to_numeric(orig_data, errors='coerce').dropna()
            synt_numeric = pd.to_numeric(synt_data, errors='coerce').dropna()
            
            if len(orig_numeric) > 0 and len(synt_numeric) > 0:
                # åˆ›å»ºç›´æ–¹å›¾
                ax.hist(orig_numeric, bins=min(20, max(5, len(orig_numeric)//10)), 
                       alpha=0.6, label='Original', density=True, color='skyblue')
                ax.hist(synt_numeric, bins=min(20, max(5, len(synt_numeric)//10)), 
                       alpha=0.6, label='Synthetic', density=True, color='lightcoral')
                
                # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
                orig_mean = orig_numeric.mean()
                synt_mean = synt_numeric.mean()
                similarity = value_analysis.get('distribution_similarity', 0) * 100
                
                ax.axvline(orig_mean, color='blue', linestyle='--', alpha=0.8, label=f'Orig Mean: {orig_mean:.2f}')
                ax.axvline(synt_mean, color='red', linestyle='--', alpha=0.8, label=f'Synt Mean: {synt_mean:.2f}')
                
                ax.legend(fontsize=8)
                ax.set_ylabel('Density', fontsize=8)
                ax.text(0.02, 0.98, f'Similarity: {similarity:.1f}%', 
                       transform=ax.transAxes, fontsize=8, verticalalignment='top',
                       bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))
            else:
                ax.text(0.5, 0.5, 'No valid numeric data', ha='center', va='center', transform=ax.transAxes)
                
        except Exception as e:
            ax.text(0.5, 0.5, f'Plot error: {str(e)[:30]}...', ha='center', va='center', transform=ax.transAxes, fontsize=8)
    
    def _plot_categorical_indicator(self, ax, indicator, value_analysis, synthetic_data):
        """ç»˜åˆ¶åˆ†ç±»å‹æŒ‡æ ‡çš„åˆ†å¸ƒå¯¹æ¯”"""
        try:
            orig_values = value_analysis.get('original_top_values', {})
            synt_values = value_analysis.get('synthetic_top_values', {})
            
            if orig_values:
                # è·å–å‰5ä¸ªæœ€å¸¸è§çš„å€¼
                top_values = list(orig_values.keys())[:5]
                
                orig_counts = [orig_values.get(val, 0) for val in top_values]
                synt_counts = [synt_values.get(val, 0) for val in top_values]
                
                x_pos = np.arange(len(top_values))
                width = 0.35
                
                ax.bar(x_pos - width/2, orig_counts, width, label='Original', alpha=0.8, color='skyblue')
                ax.bar(x_pos + width/2, synt_counts, width, label='Synthetic', alpha=0.8, color='lightcoral')
                
                # è®¾ç½®xè½´æ ‡ç­¾
                labels = [str(val)[:8] + '...' if len(str(val)) > 8 else str(val) for val in top_values]
                ax.set_xticks(x_pos)
                ax.set_xticklabels(labels, rotation=45, fontsize=8)
                
                ax.legend(fontsize=8)
                ax.set_ylabel('Count', fontsize=8)
                
                similarity = value_analysis.get('value_similarity', 0) * 100
                ax.text(0.02, 0.98, f'Similarity: {similarity:.1f}%', 
                       transform=ax.transAxes, fontsize=8, verticalalignment='top',
                       bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))
            else:
                ax.text(0.5, 0.5, 'No categorical data', ha='center', va='center', transform=ax.transAxes)
                
        except Exception as e:
            ax.text(0.5, 0.5, f'Plot error: {str(e)[:30]}...', ha='center', va='center', transform=ax.transAxes, fontsize=8)
    
    def _plot_basic_indicator_stats(self, ax, indicator, indicator_info):
        """ç»˜åˆ¶åŸºæœ¬æŒ‡æ ‡ç»Ÿè®¡"""
        try:
            orig_count = indicator_info.get('original_count', 0)
            synt_count = indicator_info.get('synthetic_count', 0)
            count_similarity = indicator_info.get('count_similarity', 0) * 100
            
            categories = ['Original', 'Synthetic']
            counts = [orig_count, synt_count]
            colors = ['skyblue', 'lightcoral']
            
            bars = ax.bar(categories, counts, color=colors, alpha=0.8)
            
            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for bar, count in zip(bars, counts):
                height = bar.get_height()
                ax.text(bar.get_x() + bar.get_width()/2., height + max(counts)*0.01,
                       f'{count}', ha='center', va='bottom', fontsize=9)
            
            ax.set_ylabel('Count', fontsize=8)
            ax.text(0.02, 0.98, f'Count Similarity: {count_similarity:.1f}%', 
                   transform=ax.transAxes, fontsize=8, verticalalignment='top',
                   bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))
                   
        except Exception as e:
            ax.text(0.5, 0.5, f'Basic plot error: {str(e)[:20]}...', ha='center', va='center', transform=ax.transAxes, fontsize=8)
    
    def _generate_multi_model_comparison_charts(self):
        """ç”Ÿæˆå¤šæ¨¡å‹å¯¹æ¯”æŸ±çŠ¶å›¾"""
        print(f"ğŸ“Š ç”Ÿæˆå¤šæ¨¡å‹å¯¹æ¯”å›¾è¡¨...")
        
        try:
            charts = []
            
            # 1. ç”ŸæˆæŒ‡æ ‡è¦†ç›–ç‡å¯¹æ¯”å›¾
            coverage_chart = self._generate_coverage_comparison_chart()
            if coverage_chart:
                charts.append(coverage_chart)
            
            # 2. ç”Ÿæˆå‰10ä¸ªæŒ‡æ ‡åˆ†å¸ƒå¯¹æ¯”å›¾
            top10_chart = self._generate_top10_indicators_comparison_chart()
            if top10_chart:
                charts.append(top10_chart)
            
            # 3. ç”Ÿæˆæ•°å€¼ç»Ÿè®¡å¯¹æ¯”å›¾
            stats_chart = self._generate_statistics_comparison_chart()
            if stats_chart:
                charts.append(stats_chart)
            
            # 4. ç”Ÿæˆæ—¶é—´æ¨¡å¼å¯¹æ¯”å›¾
            time_chart = self._generate_time_pattern_comparison_chart()
            if time_chart:
                charts.append(time_chart)
            
            print(f"âœ… ç”Ÿæˆäº† {len(charts)} ä¸ªå¤šæ¨¡å‹å¯¹æ¯”å›¾è¡¨")
            return charts
            
        except Exception as e:
            print(f"âš ï¸ å¤šæ¨¡å‹å¯¹æ¯”å›¾è¡¨ç”Ÿæˆå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return []
    
    def _generate_coverage_comparison_chart(self):
        """ç”ŸæˆæŒ‡æ ‡è¦†ç›–ç‡å¯¹æ¯”æŸ±çŠ¶å›¾"""
        try:
            if 'indicator_analysis' not in self.evaluation_results:
                return None
            
            indicator_analysis = self.evaluation_results['indicator_analysis']
            if 'models' not in indicator_analysis:
                return None
            
            # åˆ›å»ºè¦†ç›–ç‡å¯¹æ¯”å›¾
            fig, ax = plt.subplots(figsize=(12, 8))
            
            models = list(indicator_analysis['models'].keys())
            coverage_rates = []
            
            for model in models:
                model_analysis = indicator_analysis['models'][model]
                coverage_rate = model_analysis.get('coverage_similarity', 0) * 100
                coverage_rates.append(coverage_rate)
            
            bars = ax.bar(models, coverage_rates, color=['skyblue', 'lightgreen', 'lightcoral', 'gold', 'plum'][:len(models)])
            
            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for bar, rate in zip(bars, coverage_rates):
                height = bar.get_height()
                ax.text(bar.get_x() + bar.get_width()/2., height + 1,
                       f'{rate:.1f}%', ha='center', va='bottom', fontweight='bold')
            
            ax.set_title('Models Indicator Coverage Similarity Comparison', fontsize=14, fontweight='bold')
            ax.set_xlabel('Models', fontsize=12)
            ax.set_ylabel('Coverage Similarity (%)', fontsize=12)
            ax.set_ylim(0, 100)
            ax.grid(axis='y', alpha=0.3)
            
            plt.xticks(rotation=45)
            plt.tight_layout()
            
            chart_file = f'output/graph/user_{self.user_id}_coverage_comparison.png'
            plt.savefig(chart_file, dpi=300, bbox_inches='tight')
            plt.close()
            
            return chart_file
            
        except Exception as e:
            print(f"âš ï¸ è¦†ç›–ç‡å¯¹æ¯”å›¾ç”Ÿæˆå¤±è´¥: {e}")
            return None
    
    def _generate_top10_indicators_comparison_chart(self):
        """ç”Ÿæˆå‰10ä¸ªæŒ‡æ ‡åˆ†å¸ƒå¯¹æ¯”å›¾"""
        try:
            if 'indicator_analysis' not in self.evaluation_results:
                return None
            
            indicator_analysis = self.evaluation_results['indicator_analysis']
            if 'models' not in indicator_analysis:
                return None
            
            # è·å–åŸå§‹æ•°æ®å‰10ä¸ªæŒ‡æ ‡
            orig_top10 = self.user_data['indicator'].value_counts().head(10)
            orig_total = len(self.user_data)
            
            # åˆ›å»ºå¯¹æ¯”å›¾
            fig, ax = plt.subplots(figsize=(16, 10))
            
            models = list(indicator_analysis['models'].keys())
            x = np.arange(len(orig_top10))
            width = 0.15
            
            # ç»˜åˆ¶åŸå§‹æ•°æ® - è½¬æ¢ä¸ºé¢‘ç‡ç™¾åˆ†æ¯”
            orig_frequencies = (orig_top10.values / orig_total * 100)
            ax.bar(x - width*2, orig_frequencies, width, label='Original', color='darkblue', alpha=0.8)
            
            # ä¸ºæ¯ä¸ªæ¨¡å‹ç»˜åˆ¶æŸ±çŠ¶å›¾
            colors = ['skyblue', 'lightgreen', 'lightcoral', 'gold', 'plum']
            for i, model in enumerate(models):
                model_analysis = indicator_analysis['models'][model]
                synthetic_all_indicators = model_analysis.get('synthetic_all_indicators', {})
                
                # è·å–è¯¥æ¨¡å‹çš„æ€»æ•°æ®é‡
                model_total = sum(synthetic_all_indicators.values())
                
                # åŒ¹é…åŸå§‹æ•°æ®çš„æŒ‡æ ‡é¡ºåº - ä»å®Œæ•´æŒ‡æ ‡ç»Ÿè®¡ä¸­è·å–å¹¶è½¬æ¢ä¸ºé¢‘ç‡ç™¾åˆ†æ¯”
                model_counts = [synthetic_all_indicators.get(indicator, 0) for indicator in orig_top10.index]
                model_frequencies = [(count / model_total * 100) if model_total > 0 else 0 for count in model_counts]
                
                ax.bar(x + width * (i - 1), model_frequencies, width, 
                      label=model.upper(), color=colors[i % len(colors)], alpha=0.8)
            
            ax.set_title('Top 10 Indicators Frequency Distribution Comparison Across Models', fontsize=14, fontweight='bold')
            ax.set_xlabel('Indicators', fontsize=12)
            ax.set_ylabel('Frequency (%)', fontsize=12)
            ax.set_xticks(x)
            ax.set_xticklabels([ind[:15] + '...' if len(ind) > 15 else ind for ind in orig_top10.index], 
                              rotation=45, ha='right')
            ax.legend()
            ax.grid(axis='y', alpha=0.3)
            
            plt.tight_layout()
            
            chart_file = f'output/graph/user_{self.user_id}_top10_indicators_comparison.png'
            plt.savefig(chart_file, dpi=300, bbox_inches='tight')
            plt.close()
            
            return chart_file
            
        except Exception as e:
            print(f"âš ï¸ å‰10ä¸ªæŒ‡æ ‡å¯¹æ¯”å›¾ç”Ÿæˆå¤±è´¥: {e}")
            return None
    
    def _generate_statistics_comparison_chart(self):
        """ç”Ÿæˆæ•°å€¼ç»Ÿè®¡å¯¹æ¯”å›¾"""
        try:
            if 'value_analysis' not in self.evaluation_results:
                return None
            
            value_analysis = self.evaluation_results['value_analysis']
            if 'models' not in value_analysis or 'original_stats' not in value_analysis:
                return None
            
            orig_stats = value_analysis['original_stats']
            models = list(value_analysis['models'].keys())
            
            # åˆ›å»º2x2å­å›¾
            fig, axes = plt.subplots(2, 2, figsize=(16, 12))
            fig.suptitle('Statistical Measures Comparison Across Models', fontsize=16, fontweight='bold')
            
            stats = ['mean', 'std', 'min', 'max']
            stat_titles = ['Mean Values', 'Standard Deviation', 'Minimum Values', 'Maximum Values']
            
            for idx, (stat, title) in enumerate(zip(stats, stat_titles)):
                row = idx // 2
                col = idx % 2
                ax = axes[row, col]
                
                if stat in orig_stats:
                    values = [orig_stats[stat]]
                    labels = ['Original']
                    colors = ['darkblue']
                    
                    for model in models:
                        model_stats = value_analysis['models'][model].get('synthetic_stats', {})
                        if stat in model_stats:
                            values.append(model_stats[stat])
                            labels.append(model.upper())
                            colors.append(['skyblue', 'lightgreen', 'lightcoral', 'gold', 'plum'][len(values)-2])
                    
                    bars = ax.bar(labels, values, color=colors, alpha=0.8)
                    
                    # æ·»åŠ æ•°å€¼æ ‡ç­¾
                    for bar, value in zip(bars, values):
                        height = bar.get_height()
                        ax.text(bar.get_x() + bar.get_width()/2., height + max(values)*0.01,
                               f'{value:.2f}', ha='center', va='bottom', fontweight='bold')
                    
                    ax.set_title(title, fontweight='bold')
                    ax.set_ylabel(stat.title())
                    ax.grid(axis='y', alpha=0.3)
                    
                    if len(labels) > 3:
                        ax.tick_params(axis='x', rotation=45)
            
            plt.tight_layout()
            
            chart_file = f'output/graph/user_{self.user_id}_statistics_comparison.png'
            plt.savefig(chart_file, dpi=300, bbox_inches='tight')
            plt.close()
            
            return chart_file
            
        except Exception as e:
            print(f"âš ï¸ ç»Ÿè®¡å¯¹æ¯”å›¾ç”Ÿæˆå¤±è´¥: {e}")
            return None
    
    def _generate_time_pattern_comparison_chart(self):
        """ç”Ÿæˆæ—¶é—´æ¨¡å¼å¯¹æ¯”å›¾"""
        try:
            if 'time_analysis' not in self.evaluation_results:
                return None
            
            time_analysis = self.evaluation_results['time_analysis']
            if 'models' not in time_analysis or 'original_hour_dist' not in time_analysis:
                return None
            
            # åˆ›å»ºæ—¶é—´æ¨¡å¼å¯¹æ¯”å›¾
            fig, ax = plt.subplots(figsize=(14, 8))
            
            orig_hours = time_analysis['original_hour_dist']
            hours = list(range(24))
            orig_counts = [orig_hours.get(h, 0) for h in hours]
            
            # ç»˜åˆ¶åŸå§‹æ•°æ®
            ax.plot(hours, orig_counts, 'o-', linewidth=3, markersize=8, 
                   label='Original', color='darkblue')
            
            # ä¸ºæ¯ä¸ªæ¨¡å‹ç»˜åˆ¶çº¿å›¾
            colors = ['skyblue', 'lightgreen', 'lightcoral', 'gold', 'plum']
            markers = ['s', '^', 'D', 'v', 'p']
            
            for i, (model, model_time_analysis) in enumerate(time_analysis['models'].items()):
                if 'synthetic_hour_dist' in model_time_analysis:
                    synt_hours = model_time_analysis['synthetic_hour_dist']
                    synt_counts = [synt_hours.get(h, 0) for h in hours]
                    
                    ax.plot(hours, synt_counts, marker=markers[i % len(markers)], 
                           linewidth=2, markersize=6, label=model.upper(), 
                           color=colors[i % len(colors)], alpha=0.8)
            
            ax.set_title('Hourly Activity Pattern Comparison Across Models', fontsize=14, fontweight='bold')
            ax.set_xlabel('Hour of Day', fontsize=12)
            ax.set_ylabel('Activity Count', fontsize=12)
            ax.legend()
            ax.grid(True, alpha=0.3)
            ax.set_xticks(range(0, 24, 2))
            
            plt.tight_layout()
            
            chart_file = f'output/graph/user_{self.user_id}_time_pattern_comparison.png'
            plt.savefig(chart_file, dpi=300, bbox_inches='tight')
            plt.close()
            
            return chart_file
            
        except Exception as e:
            print(f"âš ï¸ æ—¶é—´æ¨¡å¼å¯¹æ¯”å›¾ç”Ÿæˆå¤±è´¥: {e}")
            return None
    
    def generate_visualizations(self):
        """ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨"""
        print(f"\nğŸ“ˆ ç”Ÿæˆåˆ†æå›¾è¡¨...")
        
        try:
            # è·å–ç¬¬ä¸€ä¸ªå¯ç”¨æ¨¡å‹ç”¨äºä¸»è¦å¯è§†åŒ–
            if not self.synthetic_datasets:
                print("âš ï¸ æ²¡æœ‰åˆæˆæ•°æ®å¯ç”¨äºå¯è§†åŒ–")
                return None
            
            first_model = list(self.synthetic_datasets.keys())[0]
            first_synthetic_data = self.synthetic_datasets[first_model]
            
            # åˆ›å»ºç»¼åˆåˆ†æå›¾è¡¨
            fig, axes = plt.subplots(2, 3, figsize=(18, 12))
            fig.suptitle(f'User {self.user_id} Comprehensive Data Analysis', fontsize=16, fontweight='bold')
            
            # 1. æŒ‡æ ‡åˆ†å¸ƒå¯¹æ¯” - ä½¿ç”¨é¢‘ç‡ç™¾åˆ†æ¯”
            if 'indicator' in self.user_data.columns:
                orig_indicators = self.user_data['indicator'].value_counts().head(10)
                synt_indicators = first_synthetic_data['indicator'].value_counts() # ä½¿ç”¨å®Œæ•´æŒ‡æ ‡ç»Ÿè®¡
                
                # è®¡ç®—é¢‘ç‡ç™¾åˆ†æ¯”
                orig_total = len(self.user_data)
                synt_total = len(first_synthetic_data)
                
                x_pos = np.arange(len(orig_indicators))
                orig_frequencies = (orig_indicators.values / orig_total * 100)
                axes[0, 0].bar(x_pos - 0.2, orig_frequencies, 0.4, label='Original', alpha=0.8)
                
                # åŒ¹é…åˆæˆæ•°æ®çš„æŒ‡æ ‡ - è½¬æ¢ä¸ºé¢‘ç‡ç™¾åˆ†æ¯”
                synt_matched = [synt_indicators.get(ind, 0) for ind in orig_indicators.index]
                synt_frequencies = [(count / synt_total * 100) if synt_total > 0 else 0 for count in synt_matched]
                axes[0, 0].bar(x_pos + 0.2, synt_frequencies, 0.4, label='Synthetic', alpha=0.8)
                
                axes[0, 0].set_title('Top 10 Indicators Frequency Distribution')
                axes[0, 0].set_xlabel('Indicators')
                axes[0, 0].set_ylabel('Frequency (%)')
                axes[0, 0].legend()
                axes[0, 0].tick_params(axis='x', rotation=45)
            
            # 2. æ•°å€¼åˆ†å¸ƒå¯¹æ¯”
            if 'value' in self.user_data.columns:
                try:
                    orig_values = pd.to_numeric(self.user_data['value'], errors='coerce').dropna()
                    synt_values = pd.to_numeric(first_synthetic_data['value'], errors='coerce').dropna() # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ¨¡å‹çš„åˆæˆæ•°æ®
                    
                    if len(orig_values) > 0 and len(synt_values) > 0:
                        axes[0, 1].hist(orig_values, bins=30, alpha=0.7, label='Original', density=True)
                        axes[0, 1].hist(synt_values, bins=30, alpha=0.7, label='Synthetic', density=True)
                        axes[0, 1].set_title('Value Distribution Comparison')
                        axes[0, 1].set_xlabel('Value')
                        axes[0, 1].set_ylabel('Density')
                        axes[0, 1].legend()
                except:
                    axes[0, 1].text(0.5, 0.5, 'Numeric Value\nAnalysis Failed', ha='center', va='center')
                    axes[0, 1].set_title('Value Distribution')
            
            # 3. æ—¶é—´åˆ†å¸ƒå¯¹æ¯”
            if 'hour' in self.user_data.columns:
                orig_hours = self.user_data['hour'].value_counts().sort_index()
                synt_hours = first_synthetic_data['hour'].value_counts().sort_index() # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ¨¡å‹çš„åˆæˆæ•°æ®
                
                hours = list(range(24))
                orig_counts = [orig_hours.get(h, 0) for h in hours]
                synt_counts = [synt_hours.get(h, 0) for h in hours]
                
                axes[0, 2].plot(hours, orig_counts, 'o-', label='Original', linewidth=2)
                axes[0, 2].plot(hours, synt_counts, 's-', label='Synthetic', linewidth=2)
                axes[0, 2].set_title('Hourly Activity Pattern')
                axes[0, 2].set_xlabel('Hour of Day')
                axes[0, 2].set_ylabel('Activity Count')
                axes[0, 2].legend()
                axes[0, 2].grid(True, alpha=0.3)
            
            # 4. æ•°æ®è´¨é‡å¯¹æ¯” - æ›´æœ‰æ„ä¹‰çš„ç›¸å¯¹æŒ‡æ ‡å¯¹æ¯”
            categories = ['Records\n(K)', 'Indicators\nCoverage (%)', 'Value Types\nCoverage (%)']
            
            # è®¡ç®—ç›¸å¯¹æŒ‡æ ‡
            orig_indicators_count = self.user_data['indicator'].nunique()
            synt_indicators_count = first_synthetic_data['indicator'].nunique()
            indicator_coverage = min(synt_indicators_count / orig_indicators_count * 100, 100) if orig_indicators_count > 0 else 0
            
            orig_values_count = self.user_data['value'].nunique() if 'value' in self.user_data.columns else 1
            synt_values_count = first_synthetic_data['value'].nunique() if 'value' in first_synthetic_data.columns else 0
            value_coverage = min(synt_values_count / orig_values_count * 100, 100) if orig_values_count > 0 else 0
            
            # æ•°æ®é‡ç”¨åƒä¸ºå•ä½ï¼Œå…¶ä»–ç”¨ç™¾åˆ†æ¯”
            original_metrics = [
                len(self.user_data) / 1000,  # è½¬æ¢ä¸ºåƒ
                100,  # åŸå§‹æ•°æ®æŒ‡æ ‡è¦†ç›–ç‡ä¸º100%
                100   # åŸå§‹æ•°æ®å€¼ç±»å‹è¦†ç›–ç‡ä¸º100%
            ]
            synthetic_metrics = [
                len(first_synthetic_data) / 1000,  # è½¬æ¢ä¸ºåƒ
                indicator_coverage,
                value_coverage
            ]
            
            x_pos = np.arange(len(categories))
            bars1 = axes[1, 0].bar(x_pos - 0.2, original_metrics, 0.4, label='Original', alpha=0.8)
            bars2 = axes[1, 0].bar(x_pos + 0.2, synthetic_metrics, 0.4, label='Synthetic', alpha=0.8)
            
            # åœ¨æŸ±çŠ¶å›¾ä¸Šæ·»åŠ æ•°å€¼æ ‡ç­¾
            for i, (bar1, bar2) in enumerate(zip(bars1, bars2)):
                if i == 0:  # Recordsç”¨Kå•ä½
                    axes[1, 0].text(bar1.get_x() + bar1.get_width()/2, bar1.get_height() + 0.1, 
                                   f'{original_metrics[i]:.1f}K', ha='center', va='bottom', fontsize=8)
                    axes[1, 0].text(bar2.get_x() + bar2.get_width()/2, bar2.get_height() + 0.1, 
                                   f'{synthetic_metrics[i]:.1f}K', ha='center', va='bottom', fontsize=8)
                else:  # å…¶ä»–ç”¨ç™¾åˆ†æ¯”
                    axes[1, 0].text(bar1.get_x() + bar1.get_width()/2, bar1.get_height() + 1, 
                                   f'{original_metrics[i]:.0f}%', ha='center', va='bottom', fontsize=8)
                    axes[1, 0].text(bar2.get_x() + bar2.get_width()/2, bar2.get_height() + 1, 
                                   f'{synthetic_metrics[i]:.0f}%', ha='center', va='bottom', fontsize=8)
            
            axes[1, 0].set_title('Data Quality Comparison')
            axes[1, 0].set_xlabel('Quality Metrics')
            axes[1, 0].set_ylabel('Value')
            axes[1, 0].set_xticks(x_pos)
            axes[1, 0].set_xticklabels(categories)
            axes[1, 0].legend()
            
            # 5. ç›¸ä¼¼æ€§åˆ†æ•°é›·è¾¾å›¾ - å¤šæ¨¡å‹å¹³å‡
            if hasattr(self, 'evaluation_results') and self.evaluation_results:
                similarity_scores = []
                labels = []
                
                # è®¡ç®—æŒ‡æ ‡è¦†ç›–ç‡å¹³å‡ç›¸ä¼¼æ€§
                if 'indicator_analysis' in self.evaluation_results and 'models' in self.evaluation_results['indicator_analysis']:
                    models = self.evaluation_results['indicator_analysis']['models']
                    coverage_scores = [model_data.get('coverage_similarity', 0) for model_data in models.values()]
                    if coverage_scores:
                        avg_coverage = sum(coverage_scores) / len(coverage_scores) * 100
                        similarity_scores.append(avg_coverage)
                        labels.append('Indicator\nCoverage')
                
                # è®¡ç®—æ•°å€¼åˆ†å¸ƒå¹³å‡ç›¸ä¼¼æ€§
                if 'value_analysis' in self.evaluation_results and 'models' in self.evaluation_results['value_analysis']:
                    models = self.evaluation_results['value_analysis']['models']
                    dist_scores = [model_data.get('distribution_similarity', 0) for model_data in models.values() if 'distribution_similarity' in model_data]
                    if dist_scores:
                        avg_dist = sum(dist_scores) / len(dist_scores) * 100
                        similarity_scores.append(avg_dist)
                        labels.append('Value\nDistribution')
                
                # è®¡ç®—æ—¶é—´æ¨¡å¼å¹³å‡ç›¸ä¼¼æ€§
                if 'time_analysis' in self.evaluation_results and 'models' in self.evaluation_results['time_analysis']:
                    models = self.evaluation_results['time_analysis']['models']
                    time_scores = [model_data.get('peak_hours_similarity', 0) for model_data in models.values()]
                    if time_scores:
                        avg_time = sum(time_scores) / len(time_scores) * 100
                        similarity_scores.append(avg_time)
                        labels.append('Time\nPattern')
                
                if similarity_scores:
                    axes[1, 1].bar(labels, similarity_scores, color=['skyblue', 'lightgreen', 'lightcoral'][:len(labels)])
                    axes[1, 1].set_title('Average Similarity Scores (%)')
                    axes[1, 1].set_ylabel('Similarity (%)')
                    axes[1, 1].set_ylim(0, 100)
                    
                    # æ·»åŠ æ•°å€¼æ ‡ç­¾
                    for i, v in enumerate(similarity_scores):
                        axes[1, 1].text(i, v + 2, f'{v:.1f}%', ha='center', va='bottom')
            
            # 6. é¢„æµ‹æ—¶é—´èŒƒå›´
            if 'predicted_date' in first_synthetic_data.columns: # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ¨¡å‹çš„åˆæˆæ•°æ®
                dates = pd.to_datetime(first_synthetic_data['predicted_date']) # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ¨¡å‹çš„åˆæˆæ•°æ®
                daily_counts = dates.dt.date.value_counts().sort_index()
                
                axes[1, 2].plot(daily_counts.index, daily_counts.values, 'o-', linewidth=2)
                axes[1, 2].set_title('Predicted Daily Activity')
                axes[1, 2].set_xlabel('Prediction Date')
                axes[1, 2].set_ylabel('Activity Count')
                axes[1, 2].tick_params(axis='x', rotation=45)
                axes[1, 2].grid(True, alpha=0.3)
            
            plt.tight_layout()
            
            # ä¿å­˜å›¾è¡¨
            chart_file = f'output/graph/user_{self.user_id}_comprehensive_analysis.png'
            plt.savefig(chart_file, dpi=300, bbox_inches='tight')
            plt.close()
            
            print(f"âœ… åˆ†æå›¾è¡¨å·²ä¿å­˜: {chart_file}")
            
            # ç”Ÿæˆå‰16ä¸ªæŒ‡æ ‡çš„è¯¦ç»†åˆ†æå›¾è¡¨
            top16_chart = self._generate_top16_indicators_chart()
            
            # ç”Ÿæˆå¤šæ¨¡å‹å¯¹æ¯”æŸ±çŠ¶å›¾
            multi_model_charts = self._generate_multi_model_comparison_charts()
            
            charts = [chart_file, top16_chart] + multi_model_charts
            return charts
            
        except Exception as e:
            print(f"âš ï¸ å›¾è¡¨ç”Ÿæˆè­¦å‘Š: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def generate_report(self):
        """ç”Ÿæˆå®Œæ•´çš„åˆ†ææŠ¥å‘Š"""
        print(f"\nğŸ“ ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š...")
        
        if not self.synthetic_datasets:
            print("âš ï¸ æ²¡æœ‰åˆæˆæ•°æ®å¯ç”¨äºç”ŸæˆæŠ¥å‘Š")
            return None
        
        first_model = list(self.synthetic_datasets.keys())[0]
        first_synthetic_data = self.synthetic_datasets[first_model]
        
        report_file = f'output/comprehensive_reports/user_{self.user_id}_analysis_report.md'
        
        with open(report_file, 'w', encoding='utf-8') as f:
            # æŠ¥å‘Šæ ‡é¢˜
            f.write(f"# ç”¨æˆ· {self.user_id} ç»¼åˆæ•°æ®åˆ†ææŠ¥å‘Š\n\n")
            f.write(f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            f.write("---\n\n")
            
            # æ‰§è¡Œæ‘˜è¦
            f.write("## ğŸ“‹ æ‰§è¡Œæ‘˜è¦\n\n")
            f.write(f"æœ¬æŠ¥å‘Šå¯¹ç”¨æˆ· {self.user_id} çš„å¥åº·æ•°æ®è¿›è¡Œäº†å…¨é¢åˆ†æï¼ŒåŒ…æ‹¬åŸå§‹æ•°æ®ç»Ÿè®¡ã€åˆæˆæ•°æ®ç”Ÿæˆã€")
            f.write(f"è´¨é‡è¯„ä¼°å’Œå¯¹æ¯”åˆ†æã€‚ä½¿ç”¨äº† {len(self.synthetic_datasets)} ä¸ªä¸åŒçš„SDVæ¨¡å‹")
            f.write(f"({', '.join(self.synthetic_datasets.keys())})ç”Ÿæˆåˆæˆæ•°æ®ï¼Œ")
            f.write(f"å¹¶å¯¹æ•°æ®çš„åˆ†å¸ƒç‰¹å¾ã€æ—¶é—´æ¨¡å¼å’ŒæŒ‡æ ‡è¦†ç›–ç‡è¿›è¡Œäº†æ·±å…¥çš„å¤šæ¨¡å‹å¯¹æ¯”åˆ†æã€‚\n\n")
            
            # æ•°æ®æ¦‚è§ˆ
            f.write("## ğŸ“Š æ•°æ®æ¦‚è§ˆ\n\n")
            f.write("### å¤šæ¨¡å‹æ•°æ®ç»Ÿè®¡\n\n")
            f.write("| æ¨¡å‹ | è®°å½•æ•° | æŒ‡æ ‡ç§ç±» | æ•°å€¼ç§ç±» | è®°å½•æ¯”ç‡ | æŒ‡æ ‡æ¯”ç‡ |\n")
            f.write("|------|--------|----------|----------|----------|----------|\n")
            
            basic_stats = self.evaluation_results.get('basic_stats', {})
            orig_records = basic_stats.get('original_records', 0)
            orig_indicators = basic_stats.get('original_indicators', 0)
            value_orig = self.user_data['value'].nunique() if 'value' in self.user_data.columns else 0
            
            # æ·»åŠ åŸå§‹æ•°æ®è¡Œ
            f.write(f"| **åŸå§‹æ•°æ®** | {orig_records:,} | {orig_indicators} | {value_orig} | - | - |\n")
            
            # ä¸ºæ¯ä¸ªæ¨¡å‹æ·»åŠ ç»Ÿè®¡
            for model_name in self.synthetic_datasets.keys():
                synt_records = basic_stats.get(f'{model_name}_records', 0)
                synt_indicators = basic_stats.get(f'{model_name}_indicators', 0)
                value_synt = self.synthetic_datasets[model_name]['value'].nunique() if 'value' in self.synthetic_datasets[model_name].columns else 0
                
                record_ratio = synt_records/orig_records if orig_records > 0 else 0
                indicator_ratio = synt_indicators/orig_indicators if orig_indicators > 0 else 0
                
                f.write(f"| {model_name.upper()} | {synt_records:,} | {synt_indicators} | {value_synt} | {record_ratio:.2f}x | {indicator_ratio:.2f}x |\n")
            
            f.write("\n")
            
            # æŒ‡æ ‡åˆ†æ
            if 'indicator_analysis' in self.evaluation_results:
                f.write("## ğŸ¯ æŒ‡æ ‡åˆ†æ\n\n")
                indicator_analysis = self.evaluation_results['indicator_analysis']
                
                f.write("### åŸå§‹æ•°æ®å‰5ä¸ªæŒ‡æ ‡\n")
                for indicator, count in indicator_analysis['original_top5'].items():
                    percentage = count / orig_records * 100
                    f.write(f"- **{indicator}**: {count} æ¬¡ ({percentage:.1f}%)\n")
                f.write("\n")
                
                # æ˜¾ç¤ºæ‰€æœ‰æ¨¡å‹çš„åˆæˆæ•°æ®æŒ‡æ ‡å¯¹æ¯”
                if 'models' in indicator_analysis and indicator_analysis['models']:
                    f.write("### å„æ¨¡å‹æŒ‡æ ‡è¦†ç›–ç‡å¯¹æ¯”\n\n")
                    f.write("| æ¨¡å‹ | å‰3ä¸ªæŒ‡æ ‡ | è¦†ç›–ç‡ç›¸ä¼¼æ€§ |\n")
                    f.write("|------|-----------|---------------|\n")
                    
                    for model_name, model_analysis in indicator_analysis['models'].items():
                        # è·å–å‰3ä¸ªæŒ‡æ ‡
                        top3_indicators = list(model_analysis['synthetic_top5'].items())[:3]
                        top3_str = ", ".join([f"{ind}({count})" for ind, count in top3_indicators])
                        
                        coverage_sim = model_analysis['coverage_similarity'] * 100
                        f.write(f"| {model_name.upper()} | {top3_str} | {coverage_sim:.1f}% |\n")
                    
                    f.write("\n")
                    
                    # æ·»åŠ å‰16ä¸ªæŒ‡æ ‡çš„å¤šæ¨¡å‹è¯¦ç»†åˆ†æ
                    f.write("### å‰16ä¸ªæœ€å¸¸è§æŒ‡æ ‡å¤šæ¨¡å‹å¯¹æ¯”åˆ†æ\n\n")
                    
                    # è·å–åŸå§‹æ•°æ®çš„å‰16ä¸ªæŒ‡æ ‡
                    orig_indicators = self.user_data['indicator'].value_counts().head(16)
                    
                    for i, (indicator, orig_count) in enumerate(orig_indicators.items(), 1):
                        f.write(f"#### {i}. {indicator} (åŸå§‹: {orig_count} æ¬¡)\n\n")
                        f.write("| æ¨¡å‹ | åˆæˆè®¡æ•° | è®¡æ•°ç›¸ä¼¼æ€§ | æ•°æ®ç±»å‹ | åˆ†å¸ƒç›¸ä¼¼æ€§ |\n")
                        f.write("|------|----------|------------|----------|------------|\n")
                        
                        for model_name, model_analysis in indicator_analysis['models'].items():
                            if 'top16_detailed_analysis' in model_analysis:
                                top16_data = model_analysis['top16_detailed_analysis']
                                if indicator in top16_data:
                                    info = top16_data[indicator]
                                    synt_count = info.get('synthetic_count', 0)
                                    count_sim = info.get('count_similarity', 0) * 100
                                    
                                    value_analysis = info.get('value_analysis', {})
                                    data_type = value_analysis.get('type', 'unknown')
                                    
                                    if data_type == 'numerical':
                                        dist_sim = value_analysis.get('distribution_similarity', 0) * 100
                                        dist_sim_str = f"{dist_sim:.1f}%"
                                    elif data_type == 'categorical':
                                        dist_sim = value_analysis.get('value_similarity', 0) * 100
                                        dist_sim_str = f"{dist_sim:.1f}%"
                                    else:
                                        dist_sim_str = "N/A"
                                    
                                    f.write(f"| {model_name.upper()} | {synt_count} | {count_sim:.1f}% | {data_type} | {dist_sim_str} |\n")
                                else:
                                    f.write(f"| {model_name.upper()} | 0 | 0.0% | N/A | N/A |\n")
                        
                        f.write("\n")
                    
                    f.write("\n")
            
            # æ•°å€¼åˆ†æ
            if 'value_analysis' in self.evaluation_results:
                f.write("## ğŸ“ˆ æ•°å€¼åˆ†æ\n\n")
                value_analysis = self.evaluation_results['value_analysis']
                
                if 'original_stats' in value_analysis and 'models' in value_analysis:
                    f.write("### å¤šæ¨¡å‹æ•°å€¼ç»Ÿè®¡å¯¹æ¯”\n\n")
                    
                    orig_stats = value_analysis['original_stats']
                    
                    # ä¸ºæ¯ä¸ªç»Ÿè®¡é‡åˆ›å»ºå¯¹æ¯”è¡¨
                    for stat in ['mean', 'std', 'min', 'max']:
                        if stat in orig_stats:
                            f.write(f"#### {stat.title()}ç»Ÿè®¡é‡å¯¹æ¯”\n\n")
                            f.write("| æ¨¡å‹ | æ•°å€¼ | ä¸åŸå§‹æ•°æ®å·®å¼‚(%) |\n")
                            f.write("|------|------|------------------|\n")
                            
                            orig_val = orig_stats[stat]
                            f.write(f"| **åŸå§‹æ•°æ®** | {orig_val:.2f} | - |\n")
                            
                            for model_name, model_value_analysis in value_analysis['models'].items():
                                if 'synthetic_stats' in model_value_analysis:
                                    synt_stats = model_value_analysis['synthetic_stats']
                                    if stat in synt_stats:
                                        synt_val = synt_stats[stat]
                                        diff_pct = abs(orig_val - synt_val) / max(abs(orig_val), 1) * 100
                                        f.write(f"| {model_name.upper()} | {synt_val:.2f} | {diff_pct:.1f}% |\n")
                            f.write("\n")
                    
                    # åˆ†å¸ƒç›¸ä¼¼æ€§å¯¹æ¯”
                    f.write("#### åˆ†å¸ƒç›¸ä¼¼æ€§å¯¹æ¯”\n\n")
                    f.write("| æ¨¡å‹ | åˆ†å¸ƒç›¸ä¼¼æ€§ | è´¨é‡ç­‰çº§ |\n")
                    f.write("|------|------------|----------|\n")
                    
                    for model_name, model_value_analysis in value_analysis['models'].items():
                        if 'distribution_similarity' in model_value_analysis:
                            dist_sim = model_value_analysis['distribution_similarity'] * 100
                            
                            if dist_sim >= 90:
                                quality = "ğŸ† å“è¶Š"
                            elif dist_sim >= 80:
                                quality = "ğŸŒŸ ä¼˜ç§€" 
                            elif dist_sim >= 70:
                                quality = "âœ… è‰¯å¥½"
                            elif dist_sim >= 60:
                                quality = "âš ï¸ ä¸€èˆ¬"
                            else:
                                quality = "âŒ éœ€æ”¹è¿›"
                            
                            f.write(f"| {model_name.upper()} | {dist_sim:.1f}% | {quality} |\n")
                    f.write("\n")
            
            # æ—¶é—´æ¨¡å¼åˆ†æ
            if 'time_analysis' in self.evaluation_results:
                f.write("## â° æ—¶é—´æ¨¡å¼åˆ†æ\n\n")
                time_analysis = self.evaluation_results['time_analysis']
                
                if 'models' in time_analysis and 'original_hour_dist' in time_analysis:
                    orig_hours = time_analysis['original_hour_dist']
                    orig_peak = max(orig_hours, key=orig_hours.get)
                    
                    f.write(f"**åŸå§‹æ•°æ®æ´»è·ƒå³°å€¼**: {orig_peak}:00 ({orig_hours[orig_peak]} æ¬¡æ´»åŠ¨)\n\n")
                    
                    f.write("### å„æ¨¡å‹æ—¶é—´æ¨¡å¼å¯¹æ¯”\n\n")
                    f.write("| æ¨¡å‹ | æ´»è·ƒå³°å€¼æ—¶é—´ | å³°å€¼æ´»åŠ¨æ•° | å³°å€¼æ—¶é—´ç›¸ä¼¼æ€§ |\n")
                    f.write("|------|--------------|------------|----------------|\n")
                    
                    for model_name, model_time_analysis in time_analysis['models'].items():
                        if 'synthetic_hour_dist' in model_time_analysis:
                            synt_hours = model_time_analysis['synthetic_hour_dist']
                            synt_peak = max(synt_hours, key=synt_hours.get)
                            peak_sim = model_time_analysis.get('peak_hours_similarity', 0) * 100
                            
                            f.write(f"| {model_name.upper()} | {synt_peak}:00 | {synt_hours[synt_peak]} | {peak_sim:.1f}% |\n")
                    f.write("\n")
            
            # æ•°æ®æ ·ä¾‹
            f.write("## ğŸ“‹ æ•°æ®æ ·ä¾‹\n\n")
            f.write("### åŸå§‹æ•°æ®æ ·ä¾‹\n")
            f.write("```\n")
            f.write(self.user_data.head(5).to_string(index=False))
            f.write("\n```\n\n")
            
            f.write("### åˆæˆæ•°æ®æ ·ä¾‹\n")
            f.write("```\n")
            display_cols = ['indicator', 'value', 'predicted_date'] if 'predicted_date' in self.synthetic_datasets[first_model].columns else self.synthetic_datasets[first_model].columns[:5] # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ¨¡å‹çš„åˆæˆæ•°æ®
            f.write(self.synthetic_datasets[first_model][display_cols].head(5).to_string(index=False))
            f.write("\n```\n\n")
            
            # è´¨é‡è¯„ä¼°ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
            f.write("## ğŸ¯ è´¨é‡è¯„ä¼°æ€»ç»“\n\n")
            f.write("### ä¸»è¦å‘ç°\n")
            
            # åŸºäºåˆ†æç»“æœç”Ÿæˆè¯„ä¼°ç»“è®º
            if 'indicator_analysis' in self.evaluation_results:
                indicator_analysis = self.evaluation_results['indicator_analysis']
                if 'models' in indicator_analysis and first_model in indicator_analysis['models']:
                    first_model_analysis = indicator_analysis['models'][first_model]
                    coverage_sim = first_model_analysis.get('coverage_similarity', 0) * 100
                    
                    if coverage_sim > 80:
                        f.write(f"âœ… **æŒ‡æ ‡è¦†ç›–ç‡ä¼˜ç§€** ({coverage_sim:.1f}%) - åˆæˆæ•°æ®å¾ˆå¥½åœ°ä¿æŒäº†åŸå§‹æ•°æ®çš„æŒ‡æ ‡å¤šæ ·æ€§\n")
                    elif coverage_sim > 60:
                        f.write(f"âš ï¸ **æŒ‡æ ‡è¦†ç›–ç‡è‰¯å¥½** ({coverage_sim:.1f}%) - å¤§éƒ¨åˆ†åŸå§‹æŒ‡æ ‡åœ¨åˆæˆæ•°æ®ä¸­å¾—åˆ°ä½“ç°\n")
                    else:
                        f.write(f"âŒ **æŒ‡æ ‡è¦†ç›–ç‡è¾ƒä½** ({coverage_sim:.1f}%) - åˆæˆæ•°æ®å¯èƒ½é—æ¼äº†ä¸€äº›é‡è¦æŒ‡æ ‡\n")
            
            if 'value_analysis' in self.evaluation_results:
                value_analysis = self.evaluation_results['value_analysis']
                if 'models' in value_analysis and first_model in value_analysis['models']:
                    first_model_value_analysis = value_analysis['models'][first_model]
                    if 'distribution_similarity' in first_model_value_analysis:
                        dist_sim = first_model_value_analysis['distribution_similarity'] * 100
                        if dist_sim > 80:
                            f.write(f"âœ… **æ•°å€¼åˆ†å¸ƒä¿çœŸåº¦é«˜** ({dist_sim:.1f}%) - åˆæˆæ•°æ®çš„ç»Ÿè®¡ç‰¹å¾ä¸åŸå§‹æ•°æ®é«˜åº¦ä¸€è‡´\n")
                        elif dist_sim > 60:
                            f.write(f"âš ï¸ **æ•°å€¼åˆ†å¸ƒåŸºæœ¬ä¿æŒ** ({dist_sim:.1f}%) - ä¸»è¦ç»Ÿè®¡ç‰¹å¾å¾—åˆ°è¾ƒå¥½ä¿ç•™\n")
                        else:
                            f.write(f"âŒ **æ•°å€¼åˆ†å¸ƒå·®å¼‚è¾ƒå¤§** ({dist_sim:.1f}%) - éœ€è¦è°ƒæ•´æ¨¡å‹å‚æ•°ä»¥æé«˜ä¿çœŸåº¦\n")
            
            if 'time_analysis' in self.evaluation_results:
                time_analysis = self.evaluation_results['time_analysis']
                if 'models' in time_analysis and first_model in time_analysis['models']:
                    first_model_time_analysis = time_analysis['models'][first_model]
                    time_sim = first_model_time_analysis.get('peak_hours_similarity', 0) * 100
                    if time_sim > 80:
                        f.write(f"âœ… **æ—¶é—´æ¨¡å¼é«˜åº¦ä¸€è‡´** ({time_sim:.1f}%) - æ´»åŠ¨å³°å€¼æ—¶é—´æ¨¡å¼å¾—åˆ°å¾ˆå¥½ä¿ç•™\n")
                    elif time_sim > 60:
                        f.write(f"âš ï¸ **æ—¶é—´æ¨¡å¼åŸºæœ¬ä¸€è‡´** ({time_sim:.1f}%) - æ•´ä½“æ´»åŠ¨è§„å¾‹ç›¸ä¼¼\n")
                    else:
                        f.write(f"âŒ **æ—¶é—´æ¨¡å¼å·®å¼‚æ˜æ˜¾** ({time_sim:.1f}%) - æ´»åŠ¨æ—¶é—´åˆ†å¸ƒæœ‰æ˜¾è‘—å˜åŒ–\n")
            
            f.write("\n")
            f.write("### å»ºè®®\n")
            f.write("- æ ¹æ®ç›¸ä¼¼æ€§åˆ†æ•°è°ƒæ•´æ¨¡å‹å‚æ•°ä»¥æé«˜æ•°æ®è´¨é‡\n")
            f.write("- å…³æ³¨æŒ‡æ ‡è¦†ç›–ç‡ï¼Œç¡®ä¿é‡è¦å¥åº·æŒ‡æ ‡ä¸è¢«é—æ¼\n")
            f.write("- éªŒè¯æ—¶é—´æ¨¡å¼çš„åˆç†æ€§ï¼Œä¿æŒç”¨æˆ·çš„æ´»åŠ¨è§„å¾‹\n")
            f.write("- å®šæœŸè¯„ä¼°åˆæˆæ•°æ®è´¨é‡ï¼ŒæŒç»­ä¼˜åŒ–ç”Ÿæˆæ•ˆæœ\n\n")
            
            # æŠ€æœ¯ä¿¡æ¯
            f.write("## ğŸ”§ æŠ€æœ¯ä¿¡æ¯\n\n")
            f.write(f"- **æ¨¡å‹ç±»å‹**: PAR (Probabilistic AutoRegressive)\n")
            f.write(f"- **è®­ç»ƒæ•°æ®é‡**: {orig_records:,} æ¡è®°å½•\n")
            f.write(f"- **ç”Ÿæˆæ•°æ®é‡**: {synt_records:,} æ¡è®°å½•\n")
            f.write(f"- **æ•°æ®æ—¶é—´èŒƒå›´**: {(self.last_date - timedelta(days=30)).date()} åˆ° {self.last_date.date()}\n")
            f.write(f"- **æŠ¥å‘Šç”Ÿæˆå·¥å…·**: ComprehensiveEvaluator v1.0\n\n")
            
            f.write("---\n")
            f.write("*æ­¤æŠ¥å‘Šç”±SDV-Thetaç»¼åˆè¯„æµ‹å™¨è‡ªåŠ¨ç”Ÿæˆ*")
        
        print(f"âœ… åˆ†ææŠ¥å‘Šå·²ä¿å­˜: {report_file}")
        
        # åŒæ—¶ä¿å­˜æ•°æ®æ–‡ä»¶
        data_file = f'output/comprehensive_reports/user_{self.user_id}_synthetic_data.csv'
        self.synthetic_datasets[first_model].to_csv(data_file, index=False) # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ¨¡å‹çš„åˆæˆæ•°æ®
        print(f"âœ… åˆæˆæ•°æ®å·²ä¿å­˜: {data_file}")
        
        return report_file

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ SDV-Theta ç»¼åˆæ•°æ®è¯„æµ‹å™¨ - åˆ†æå’ŒæŠ¥å‘Šç”Ÿæˆ")
    print("=" * 60)
    
    # æ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°
    if len(sys.argv) < 2:
        print("ğŸ“– ä½¿ç”¨æ–¹æ³•:")
        print(f"  python3 {sys.argv[0]} <ç”¨æˆ·ID> [æ¨¡å‹åˆ—è¡¨] [æœ€å¤§è®°å½•æ•°]")
        print(f"\nğŸ’¡ ç¤ºä¾‹:")
        print(f"  python3 {sys.argv[0]} 169                    # ç”¨æˆ·169ï¼Œåˆ†ææ‰€æœ‰å¯ç”¨æ¨¡å‹")
        print(f"  python3 {sys.argv[0]} 169 ctgan,par         # ç”¨æˆ·169ï¼Œåªåˆ†æCTGANå’ŒPARæ¨¡å‹")
        print(f"  python3 {sys.argv[0]} 169 all 5000          # ç”¨æˆ·169ï¼Œæ‰€æœ‰æ¨¡å‹ï¼Œæœ€å¤š5000æ¡è®°å½•")
        print(f"\nğŸ“‹ æ”¯æŒçš„æ¨¡å‹: gaussian_copula, ctgan, copulagan, tvae, par")
        print(f"\nâš ï¸  æ³¨æ„: è¯·å…ˆè¿è¡Œ python3 run_all_models.py ç”Ÿæˆåˆæˆæ•°æ®")
        
        # æ˜¾ç¤ºå¯ç”¨ç”¨æˆ·
        evaluator = ComprehensiveEvaluator()
        evaluator.load_data()
        evaluator.analyze_users()
        return
    
    # è§£æå‚æ•°
    try:
        user_id = int(sys.argv[1])
        models_arg = sys.argv[2] if len(sys.argv) > 2 else 'all'
        max_records = int(sys.argv[3]) if len(sys.argv) > 3 else None
        
        # è§£ææ¨¡å‹åˆ—è¡¨
        if models_arg.lower() == 'all':
            models = None  # ä½¿ç”¨æ‰€æœ‰æ”¯æŒçš„æ¨¡å‹
        else:
            models = [m.strip() for m in models_arg.split(',')]
            # éªŒè¯æ¨¡å‹åç§°
            valid_models = ['gaussian_copula', 'ctgan', 'copulagan', 'tvae', 'par']
            invalid_models = [m for m in models if m not in valid_models]
            if invalid_models:
                print(f"âŒ æ— æ•ˆçš„æ¨¡å‹åç§°: {invalid_models}")
                print(f"ğŸ“‹ æ”¯æŒçš„æ¨¡å‹: {valid_models}")
                return
        
        if max_records and max_records < 100:
            print("âŒ æœ€å¤§è®°å½•æ•°å¿…é¡»å¤§äº100")
            return
            
    except ValueError:
        print("âŒ ç”¨æˆ·IDå¿…é¡»æ˜¯æ•°å­—")
        return
    
    # æ‰§è¡Œç»¼åˆè¯„æµ‹
    evaluator = ComprehensiveEvaluator()
    
    # 1. æ•°æ®åŠ è½½å’Œç”¨æˆ·é€‰æ‹©
    if not evaluator.load_data():
        return
    
    if not evaluator.select_user(user_id, max_records=max_records):
        return
    
    # 2. åŠ è½½åˆæˆæ•°æ®
    loaded_models = evaluator.load_synthetic_data(models=models)
    if not loaded_models:
        return
    
    # 3. ç»Ÿè®¡åˆ†æ
    evaluator.statistical_analysis()
    
    # 4. ç”Ÿæˆå¯è§†åŒ–
    chart_files = evaluator.generate_visualizations()
    
    # 5. ç”ŸæˆæŠ¥å‘Š
    report_file = evaluator.generate_report()
    
    # 6. æ€»ç»“
    print(f"\nğŸ‰ ç”¨æˆ· {user_id} ç»¼åˆè¯„æµ‹å®Œæˆï¼")
    print(f"ğŸ“Š åˆ†æçš„æ¨¡å‹: {', '.join(loaded_models)}")
    if isinstance(chart_files, list) and chart_files:
        print(f"\nğŸ“Š ç”Ÿæˆçš„å›¾è¡¨æ–‡ä»¶:")
        for i, chart_file in enumerate(chart_files, 1):
            if chart_file:
                chart_name = chart_file.split('/')[-1]
                if 'comprehensive_analysis' in chart_name:
                    print(f"  {i}. ç»¼åˆåˆ†æå›¾è¡¨: {chart_file}")
                elif 'top16_indicators' in chart_name:
                    print(f"  {i}. å‰16ä¸ªæŒ‡æ ‡è¯¦ç»†åˆ†æ: {chart_file}")
                elif 'coverage_comparison' in chart_name:
                    print(f"  {i}. æŒ‡æ ‡è¦†ç›–ç‡å¯¹æ¯”å›¾: {chart_file}")
                elif 'top10_indicators_comparison' in chart_name:
                    print(f"  {i}. å‰10ä¸ªæŒ‡æ ‡åˆ†å¸ƒå¯¹æ¯”å›¾: {chart_file}")
                elif 'statistics_comparison' in chart_name:
                    print(f"  {i}. æ•°å€¼ç»Ÿè®¡å¯¹æ¯”å›¾: {chart_file}")
                elif 'time_pattern_comparison' in chart_name:
                    print(f"  {i}. æ—¶é—´æ¨¡å¼å¯¹æ¯”å›¾: {chart_file}")
                else:
                    print(f"  {i}. å…¶ä»–å›¾è¡¨: {chart_file}")
    elif chart_files:
        print(f"ğŸ“Š åˆ†æå›¾è¡¨: {chart_files}")
    
    print(f"ğŸ“„ åˆ†ææŠ¥å‘Š: {report_file}")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: output/comprehensive_reports/")
    print(f"ğŸ“ å›¾è¡¨ç›®å½•: output/graph/")
    print(f"\nğŸ’¡ å¦‚éœ€é‡æ–°ç”Ÿæˆåˆæˆæ•°æ®ï¼Œè¯·è¿è¡Œ: python3 run_all_models.py")

if __name__ == "__main__":
    main() 